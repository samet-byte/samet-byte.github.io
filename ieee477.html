<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Microsoft Word - IEEE_477.doc</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 16pt; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s2 { color: #954F72; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s3 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s4 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s6 { color: black; font-family:TimesNewRomanPS-BoldItalicMT, serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s8 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s10 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s11 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s13 { color: #0563C1; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s14 { color: #0563C1; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s15 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s16 { color: #0563C1; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s17 { color: #0563C1; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .a, a { color: #0563C1; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: d1 1; }
 #l2> li>*:first-child:before {counter-increment: d1; content: counter(d1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l3 {padding-left: 0pt;counter-reset: c2 1; }
 #l3> li>*:first-child:before {counter-increment: c2; content: counter(c2, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l4 {padding-left: 0pt;counter-reset: e1 1; }
 #l4> li>*:first-child:before {counter-increment: e1; content: counter(e1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 li {display: block; }
 #l5 {padding-left: 0pt;counter-reset: f1 1; }
 #l5> li>*:first-child:before {counter-increment: f1; content: "["counter(f1, decimal)"] "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l5> li:first-child>*:first-child:before {counter-increment: f1 0;  }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 4pt;padding-left: 16pt;text-indent: 0pt;text-align: center;">A COMPREHENSIVE REVIEW OF FACE DETECTION, TRACKING &amp; FACE SWAP ALGORITHMS WITH MATLAB APPLICATIONS</h1><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="0" alt="image" src="IEEE_477_files/Image_001.png"/></span></p><p class="s1" style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: center;">Bayat, Samet<span class="s2">1</span></p><p class="s3" style="padding-top: 5pt;padding-left: 156pt;text-indent: 0pt;text-align: center;">Electrical-Electronics Engineering, Baskent University Turkey</p><p style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: center;"><a href="mailto:22293730@mail.baskent.edu.tr" class="s4" target="_blank">1</a><a href="mailto:22293730@mail.baskent.edu.tr" class="s5" target="_blank">22293730@mail.baskent.edu.tr</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Abstract<span class="h2">— In this project work, face/feature detection, feature tracking and face swap applications will be handled and evaluated with Viola Jones, KLT and MPB algorithms. The work includes three different applications: &#39;Real Time Face Tracking&#39;,   &#39;Image   Editing/Blending&#39;   and   &#39;Video Editing/Blending&#39;. The open-source content of Mahmoud Afifi (York University) is used as the video processing dataset and MATLAB code guide. The aim is to examine the algorithms in detail,  how  MATLAB  used  it  and  to  determine  the proportionality  of  the  application-theoretical  knowledge common shares.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Keywords<span class="s7">— </span><span class="h2">Viola Jones, KLT, MPB, Face Swap, MATLAB.</span></p><ol id="l1"><li data-list-text="I."><p style="padding-top: 9pt;padding-left: 107pt;text-indent: -14pt;text-align: left;">Introduction</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 13pt;text-align: justify;">‘Digital masks/filters’ became vouge of our virtual world and even affect in our attitudes in non-virtual areas. [1,2] In my work, I examined the reflection of this socio-cultural fact and trend on the computer age with some algorithms and case studies as image/video face swap and filtering applications which  was  made  using  Viola  Jones  Algorithm,  KLT (Kanade–Lucas–Tomasi feature tracker) and MPB (Modified Poisson blending technique). Face manipulation concept is so wide, in literature there are many different approaches to achieve this problem. In common, the blending process progresses with the mask which taken from source applies to target’s face. Unlike Face2Face (3D masks), in our concept ‘Face Swap’, we use 2-dimensional masks (projections). On the other hand, <i>deepfake </i>refers to more complex applications. In deepfake approaches, generally GAN or Autoencoder-like deep learning applications are used. [3-5]</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="II."><p style="padding-top: 7pt;padding-left: 31pt;text-indent: -14pt;text-align: left;">Behind the Scenes: Algorithms &amp; Math Behind</p><p style="padding-left: 108pt;text-indent: 0pt;text-align: left;">Applications</p><ol id="l2"><li data-list-text="A."><p class="s3" style="padding-top: 7pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Viola Jones Algorithm (for Face &amp; Feature Detection)</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">Despite its introduction at the beginning of the millennium, Viola Jones Algorithm is still one of the most popular face detection algorithms. It classifies values of images based on the simple ‘features’ rather than pixels. The main reason to do that is feature-based system operates much faster than a pixel- based system. At this point Haar basis functions and Integral Image help us to detect face(s) and simple features, for this process we use three kinds of Haar-like features as Edge-like, line-like and four-rectangle(diagonal) which can be seen, respectively, in Figure 1. [6,7]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span><img width="200" height="35" alt="image" src="IEEE_477_files/Image_002.jpg"/></span></p><p class="s8" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Figure 1. Haar-like features.</p><p class="s8" style="padding-left: 16pt;text-indent: 0pt;line-height: 12pt;text-align: center;">From left: Line-like, edge-like, diagonal features<span class="s9">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">The process that makes this face recognition algorithm faster than others is Integral Image. It allows to compute related part of image much faster with less variables compared to pixel-to-pixel computation. A simple sum operation, where ii(x, y) is the integral image and i(x, y) is the original image, can be used for this process: [6]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 71pt;text-indent: 0pt;text-align: left;"><span><img width="171" height="35" alt="image" src="IEEE_477_files/Image_003.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">On MATLAB side, the most proper and fastest solution for this particular case is <i>vision.CascadeObjectDetector()</i>. This 1 line of code not just provides the integral image, it nearly includes all the methods discussed above and will be discuss below. (Haar-basis functions, cascading etc.)[8]</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">Other important algorithm for more accurate results is Adaptive Boosting (AdaBoost) Algorithm. It refers the process which combines several weak learners (for our case, weak learner refers to each facial organs: nose, left eye, mouth, etc.) into a strong learner called boosting. (This process could be considered as complementary of Cascading) Key point here is training period lasts sequentially. [9]</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">After we found the rough position of face, cascade filter (it could be considered as binary classifier) searches a particular niche feature (an eye, ear, mouth, etc.) During search, if it detects a particular feature (ex. Right eye) it continues to search another feature. (ex. nose) If it gets always positive responses (finds desired features): finally, it returns the ‘ROI’ (Region of Interest) of face with locations of features. If it cannot find desired feature: it stops the process (Reduces the amount of computation time spent on false, windows) retries the process on different area of image. [6]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;text-align: left;"><span><img width="177" height="111" alt="image" src="IEEE_477_files/Image_004.jpg"/></span></p><p class="s8" style="padding-left: 16pt;text-indent: 0pt;text-align: center;">Figure 2. Schematic depiction of the detection cascade.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">During trials, machine found 175 out of 197 potential heads, and 9 non-heads were classified as head. (Since we work with single-head models for face swapping applications, the margin of error for multi-head models is negligible.) Accuracy  =  0.8495145631  <span class="s10">≈  </span>84.95%.  One  of  the possibilities  behind  that  failure  might  be  the  default ‘threshold’ value. In the work below, we can observe how threshold effects the result:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;text-align: left;"><span><img width="263" height="101" alt="image" src="IEEE_477_files/Image_005.png"/></span></p><p class="s8" style="padding-left: 48pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Figure 3. From left: (a) Threshold:2, (b) Threshold:12,</p><p class="s8" style="padding-left: 48pt;text-indent: 0pt;line-height: 9pt;text-align: left;">(c) Threshold:22, (Face model: Alejandro Jodorovsky)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">Finally, if we examine the average behavior of algorithm results shows that, when the non-face area decreases, face detection lasts quicker:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">0.164459 (for 1 face) = 0.164459 sec / face</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">0.423240 (for 29 face and 6 errors) = 0.014594 sec / face</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">1.207437 (for 133 face and 2 errors) = 0.009078 sec / face.</p></li><li data-list-text="B."><p class="s3" style="padding-top: 7pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Kanade–Lucas–Tomasi (KLT) Feature Tracker</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">KLT Algorithm is capable of track objects with many methods,  varieties,  different  motions,  for  face  swap application we need simply ‘single object tracking’ concept which can be obtained with ‘Harris Corners.’[10]</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">Harris corners allows us to detect corners in image. Harris detector evaluates eigenvalues to find the &#39;ROI&#39;. The process simply explained with figures below: [11]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="126" alt="image" src="IEEE_477_files/Image_006.jpg"/></span></p><p class="s8" style="padding-top: 1pt;padding-left: 83pt;text-indent: 0pt;text-align: left;">Figure 4. Harris Corner concept.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">The main difference between Harris detector-based feature tracking and ours (KLT Shi-Thomasi, 1994) is that our version uses just edges as good-features to track face. It obviously increases the error ratio but also speed up process significantly. Since we use these features in video processing, keeping the number of edges optimal will be much more efficient for the video editing process.</p><p style="padding-left: 5pt;text-indent: 15pt;text-align: justify;">In MATLAB, <i>‘detectMinEigenFeatures’ </i>finds the Harris corners from eigenvalues and uses ‘boundarybox’ (area separated from the rest of the image) for tracking and ‘<i>vision. PointTracker’ </i>tracks the related part frame by frame. [12]</p></li><li data-list-text="C."><p class="s3" style="padding-top: 4pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Image Preprocessing (Image Alignment &amp; Stabilization)</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">Image alignment is the procedure of the overlay of the images of same scene under various conditions, such as from various viewpoints, with different illumination, using a variety of the sensors, or at various times. Image alignment is transforming a source image to the coordinate system of the reference image besides image stabilization helps us to fix issues caused by blur (problems occur during exposure because of camera motion). [13]</p></li><li data-list-text="D."><p class="s3" style="padding-top: 7pt;padding-left: 22pt;text-indent: -16pt;text-align: justify;">Modified Poisson Blending Tecnique (MPB)</p></li></ol><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">The most relevant part of this work with image/video editing is definitely the MPB phase. MPB also called as <i>gradient  image  editing.  </i>Masking  application  which mentioned at the very beginning of the introduction takes its final shape with this technique. It primarily prevents the bleeding problems in operation. Key factor here is the source and target <i>ROI </i>have to be well aligned. Deviation in alignment process may cause crucial errors at this part.</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">Gradient images and the Laplace filter in the blending phase allow the process to be both quick and consistent. The gradient of an image measures how it is changing. It provides two pieces of information. The magnitude of the gradient tells us how quickly the image is changing, while the direction of the gradient tells us about the direction in which the image is changing most rapidly.</p></li><li data-list-text="III."><p style="padding-top: 9pt;padding-left: 118pt;text-indent: -14pt;text-align: left;">Dataset</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">At the very beginning of coding, for feature training, it tried to evaluate feature training stage with external dataset. In my trials, I tried to use ‘MS Celeb 1M’ [15] dataset which contains 8,456,240 images yet due to some restrictions of operating system and graphics card the process could not be completed  with  desired  conditions.  Therefore,  I  used ‘Transfer Learning’ concept for feature detection phase, from OpenCV  libraries,  ‘haarcascade_frontalface_alt.xml’  file transferred which includes feature detection information about frontal face. [14,15] Here machine evaluates that information with using Haar-like Features and cascading. In addition to this, I also use built in MATLAB Haar-Cascade module called ‘<i>FrontalFaceCART’</i>.</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">For video-face swap, there are 3 source and target frame sets exist in original dataset. In all samples which are stabilized and aligned, target donors face the lens at a 90- degree angle. We observe that oscillation and zoom in-out are done without disturbing the angle between their faces and the lens (without turning their heads). As target donor, many trials made with various videos, yet results were unexpected. Attempts with wrong results showed at the end of this part.</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">Unlike image editing, variables change constantly in video editing. (Face/feature location, lighting, rotation, etc.) Before the editing process, we know from previous works that aligning and stabilization has critical importance for face swap application. Therefore, both target and source images need preprocessing. The very first step is converting videos to frames. A simple ffmpeg terminal command used for converting. Later, thanks for M. Afifi’s stabilization tool, 4<span class="s11">th</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">target’s stabilized video (where the author is the target donor) obtained about in 5 mins.</p></li><li data-list-text="IV."><p style="padding-top: 9pt;padding-left: 22pt;text-indent: -14pt;text-align: justify;">Image &amp; Video Face Swap Applications In MATLAB</p><ol id="l3"><li data-list-text="A."><p class="s3" style="padding-top: 7pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Real-time Face Detection and Tracking</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">Here an environment with MATLAB built-in functions and transfer learning concept created. (Instead of training, I used existed data that includes needed information) In a given period, machine constantly takes pictures, converts into grayscale, evaluates the face and feature locations and projects to user.</p><p style="padding-left: 5pt;text-indent: 13pt;text-align: justify;">Main problems detected: In dim light, when shooting from a side profile, when accessories are attached (eg eyeglasses), the machine has occasionally calculated incorrect outputs.</p></li><li data-list-text="B."><p class="s3" style="padding-top: 7pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Poisson Image Editing (Face &amp; Object Swap / Image)</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">Poisson  Image  Editing  (which  includes  Poisson distribution,  Laplace  filtering  stages)  was  the  second application. Here user crops (creates a mask) the related part of image. (It is not have to be face) The main problem here was  scaling  issues.  The  issue  solved  with  <i>‘imresize’ </i>command and a short trial-error period but still for further implementations to get proper solutions, program needs much efficient way to automate the aligning process. (<i>imresize </i>function and intermediate steps effects the dimensions and resolutions of output)</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><span><img width="329" height="65" alt="image" src="IEEE_477_files/Image_007.jpg"/></span></p><p class="s8" style="padding-left: 16pt;text-indent: 0pt;text-align: center;">Figure 5. From left: (a) source, (b) target, (c) mask, (d) mask placed in</p><p class="s8" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: center;">ROI by user (e) output.</p><p class="s8" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Source Image: The Office, 2005.</p><p class="s8" style="padding-left: 16pt;text-indent: 0pt;line-height: 12pt;text-align: center;">Target Image: The Wolf of Wall Street, 2013<span class="s9">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="C."><p class="s3" style="padding-top: 6pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Video Editing / Face Swap Application</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">The  combination  of  <i>Real-time  &amp;  Image  Editing </i>applications became an automated video face swap technique here. Steps used in application first imported here. Primarily, image  converted  to  grayscale  for  faster  detection. Additionally,  for  more  advanced  feature  extraction, haarcascade_frontalface_alt.xml file from OpenCV is used as transfer learning method.</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">MPB function which developed from application <i>Image Editing</i>, blends target and source with using Poisson distribution and Laplace filters as these equations indicated. After computation, function saves blended frame to output folder. Unlike application ii, here since we work with aligned and stabilized dataset, we do not need to rearrange target or source frame. When KLT is added here, the blending process will be applied to the &#39;ROI&#39; in a fully automatic manner during the process.</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">Again, like in real-time tracking, machine creates edge points with using eigenvalues and calculates the new head position based on the previous frame. During operations, masks and the target frame are constantly refreshed until it</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">reaches the target number of frames. The process can be observed with the simple user interface that pops-up when the code executed.</p><p style="padding-left: 36pt;text-indent: 0pt;text-align: left;"><span><img width="250" height="107" alt="image" src="IEEE_477_files/Image_008.png"/></span></p><p class="s8" style="padding-left: 42pt;text-indent: 0pt;text-align: left;">Figure 6. From left: (a) target, (b) source mask, (c) output.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="V."><p style="padding-left: 82pt;text-indent: -14pt;text-align: left;">Exemplary Applications</p><ol id="l4"><li data-list-text="A."><p class="s3" style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;text-align: justify;">High-Resolution Neural Face Swapping for Visual Effects</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">Unlike our mid-resolution examples, in 2020, Disney released an Autoencoder &amp; GAN-based method that allows neural face swap for high-resolution videos. (Let&#39;s note that, for a single relatively high-resolution image in our custom trial, the process took 18 minutes.) [7]</p></li><li data-list-text="B."><p class="s3" style="padding-top: 7pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Synthetic Film Dubbing</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">Nowadays, in the age of video consumption, many people prefer  to  watch  dubbed  content  rather  than  subtitles. Companies like &#39;Flawless AI&#39;, which thinks that following the subtitles disconnects the viewer from the story, aims to overcome this situation by making lip sync based visual translation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="C."><p class="s3" style="padding-top: 6pt;padding-left: 20pt;text-indent: -14pt;text-align: justify;">Deepfake Anchor</p></li></ol><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">Channel owners are looking for low-budget solutions these days when costs are increasing. &#39;Deepfake anchors&#39; seems to be the new trend for news channels in the field of video processing, where artificial intelligence is in demand. The process, on the other hand, is similar to the stages mentioned in our study, but the masking process is more detailed and audio processing is also included in the process.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="VI."><p style="padding-top: 8pt;padding-left: 105pt;text-indent: -14pt;text-align: left;">Ethical Issues</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">The vast majority of deepfake applications are used for entertainment (as individual and as industrial) and education, but in some cases, we see this application used maliciously. In the video published recently, the &#39;fake&#39; president who called for his army to &#39;surrender&#39; during the Russia-Ukraine war, and Obama, who was also one of the bureaucratic donors, &#39;saying unspeakable words&#39; are examples of these. For such applications of rapidly developing artificial intelligence, states should keep sanctions tight. Also, some recent applications like FaceApp, ZAO, Reface.. violates personal data protection law yet due to delay of international laws, solving such situations requires a long time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="VII."><p style="padding-top: 4pt;padding-left: 119pt;text-indent: -36pt;text-align: left;">C<span class="s12">ONCLUSIONS</span></p></li></ol><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">When we complete the applications, we see that some of the results developed as we wanted, others could not reach the desired situation. &#39;Alpha Composition - Transparency Issue&#39;, which is corrected for Image Editing, and our code that requires less trail -error is good for us. For video editing, semi</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">-automation was provided much faster even for the end user. The biggest success here is &#39;Tracking Optimization&#39;. The object followed by random features selected from the first image can now be followed more stable with the help of &#39;Good Features&#39;. At the beginning of the factors behind this success, the &#39;Transfer Learning&#39; method is coming. Thanks to the feature information in the .xml file (which is our &#39;Viola- Jones Algorihtm&#39;), we can detect a face by 84.95%.</p><p style="padding-left: 5pt;text-indent: 10pt;text-align: justify;">On the other hand, some situations during the process did not result as desired. The dataset we used for &#39;Feature Training&#39; has been subjected to GPU-operational system restrictions. Although improvements have been made for image editing, which we could perform manually, full automation could not be realized. (We still need to watch the Trial-Error for the target and resource aligning process. The results we improve with the &#39;Good Features to Track&#39; approach are also subject to the Trial-Error process as for image editing. Our code, which is requested to work like an optimized program (can make face swap for each appropriate video) is not yet at this stage. (For this reason, for the appropriate results in video trials, the dataset used by Mahmoud Afifi was used.) For the result obtained with the video added to the existing Dataset (the face of the target author), appropriate results were obtained in only 9 seconds of 1 minute 33 seconds footage. In addition, MATLAB Android Toolbox allows us to mobilize our application, for further it can be adapted to the phone.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: 0pt;text-align: center;">Acknowledgment</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 10pt;text-align: justify;">I would like to thank Begüm Cangöz, who laid the foundation for how I should look at algorithms and did not spare me her opinions, M. Afifi, who helped us learn by sharing the open-source code and dataset, and my mother, who has maintained her support and optimistic attitude from the beginning of my project, no matter how bad the obtained outputs were.</p><p style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: center;">References</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l5"><li data-list-text="[1]"><p class="s13" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">https://thesocialshepherd.com/blog/snapchat-statistics<span class="s14"> </span><span class="s8">Accessed: Dec 18, 2022.</span></p></li><li data-list-text="[2]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">Habib A, Ali T, Nazir Z, Mahfooz A. Snapchat filters changing young women&#39;s attitudes. Ann Med Surg (Lond). 2022 Sep 17;82:104668. doi:  10.1016/j.amsu.2022.104668.  PMID:  36268310;  PMCID: PMC9577667.</p></li><li data-list-text="[3]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. 2018. Face2Face: real-time face capture and reenactment of RGB videos. Commun. ACM 62, 1 (January 2019), 96–104. <span class="s13">https://doi.org/10.1145/3292039</span></p></li><li data-list-text="[4]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">Naruniec, J. &amp; Helminger, L. &amp; Schroers, C. &amp; Weber, R.M.. (2020). High<span class="s15">-</span>Resolution Neural Face Swapping for Visual Effects. Computer Graphics Forum. 39. 173-184. 10.1111/cgf.14062.</p></li><li data-list-text="[5]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">Afifi, Mahmoud, et al. “Video Face Replacement System Using a Modified  Poisson  Blending  Technique.”  2014  International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), IEEE, 2014, doi:10.1109/ispacs.2014.7024453.</p></li><li data-list-text="[6]"><p class="s16" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;"><span class="s8">Viola, P.; Jones, M. (2001). </span>&quot;Rapid object detection using a boosted<span class="s17"> </span>cascade of simple features&quot;<span class="s8">. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001. IEEE Comput. Soc. 1.</span></p></li><li data-list-text="[7]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">C. Papageorgiou, M. Oren, and T. Poggio. A general framework for object detection. In International Conference on Computer Vision, 1998.</p></li><li data-list-text="[8]"><p style="padding-left: 27pt;text-indent: -21pt;line-height: 9pt;text-align: justify;"><a href="http://www.mathworks.com/help/vision/ref/vision.cascadeobjectdetec" class="a" target="_blank">https://</a><a href="http://www.mathworks.com/help/vision/ref/vision.cascadeobjectdetec" target="_blank">www.mathworks.com/help/vision/ref/vision.cascadeobjectdetec</a></p><p class="s13" style="padding-left: 27pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">tor-system-object.html<span class="s14"> </span><span class="s8">Accessed: Nov 28, 2022.</span></p></li><li data-list-text="[9]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems by Aurelien Geron (pg.201-205)</p></li><li data-list-text="[10]"><p style="padding-left: 27pt;text-indent: -21pt;line-height: 9pt;text-align: justify;"><a href="http://www.crcv.ucf.edu/wp-content/uploads/2019/03/Lecture-10-" class="a" target="_blank">https://</a><a href="http://www.crcv.ucf.edu/wp-content/uploads/2019/03/Lecture-10-" target="_blank">www.crcv.ucf.edu/wp-content/uploads/2019/03/Lecture-10-</a></p><p class="s13" style="padding-left: 27pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">KLT.pdf <span class="s8">Accessed: Nov 30, 2022.</span></p></li><li data-list-text="[11]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">Chris Harris and Mike Stephens (1988). &quot;A Combined Corner and Edge Detector&quot;. Alvey Vision Conference. Vol. 15.</p></li><li data-list-text="[12]"><p style="padding-left: 27pt;text-indent: -21pt;line-height: 9pt;text-align: justify;"><a href="http://www.mathworks.com/help/vision/ref/detectmineigenfeatures.ht" class="a" target="_blank">https://</a><a href="http://www.mathworks.com/help/vision/ref/detectmineigenfeatures.ht" target="_blank">www.mathworks.com/help/vision/ref/detectmineigenfeatures.ht</a></p><p class="s13" style="padding-left: 27pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">ml<span class="s14"> </span><span class="s8">Accessed: Dec 5, 2022.</span></p></li><li data-list-text="[13]"><p class="s8" style="padding-left: 27pt;text-indent: -21pt;text-align: justify;">Ghindawi, Ikhlas &amp; Abdulateef, Sali &amp; Dawood, Amaal &amp; yousif, Intisar. (2020). Modified Alignment Technique Using Matched Important Features. International Journal of Engineering Research and Advanced Technology. 06. 01-06. 10.31695/IJERAT.2020.3592.</p></li><li data-list-text="[14]"><p class="s13" style="padding-left: 27pt;text-indent: -21pt;line-height: 9pt;text-align: justify;">https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html</p><p class="s8" style="padding-left: 27pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Accessed: Dec 13, 2022.</p></li><li data-list-text="[15]"><p class="s13" style="padding-left: 27pt;text-indent: -21pt;line-height: 9pt;text-align: justify;">https://github.com/opencv/opencv/tree/master/data/haarcascades</p></li></ol><p class="s8" style="padding-left: 27pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Accessed: Dec 13, 2022.</p></body></html>
