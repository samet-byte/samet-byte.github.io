<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Report_Samet_Bayat_22293730_final</title><meta name="author" content="SAMET BAYAT"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 h3 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .p, p { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s1 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s2 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s3 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt; }
 .s4 { color: #292929; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s5 { color: black; font-family:"Courier New", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -5pt; }
 .s7 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s8 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -3pt; }
 h4 { color: black; font-family:CourierNewPS-BoldItalicMT, serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .s9 { color: black; font-family:"Courier New", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s10 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .s11 { color: #404040; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s13 { color: #292929; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s14 { color: #292929; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s15 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s16 { color: #0D00FF; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 h2 { color: #FFF; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s21 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s22 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s23 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s24 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .a, a { color: #0462C1; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s26 { color: #0462C1; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s27 { color: #0462C1; font-family:"Courier New", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s29 { color: #0462C1; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s30 { color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, upper-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c1, upper-roman)"."counter(c2, lower-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l3 {padding-left: 0pt;counter-reset: d1 2; }
 #l3> li>*:first-child:before {counter-increment: d1; content: counter(d1, upper-roman)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l3> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l4 {padding-left: 0pt;counter-reset: d2 4; }
 #l4> li>*:first-child:before {counter-increment: d2; content: counter(d1, upper-roman)"."counter(d2, lower-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: d2 0;  }
 #l5 {padding-left: 0pt;counter-reset: d3 1; }
 #l5> li>*:first-child:before {counter-increment: d3; content: counter(d1, upper-roman)"."counter(d2, lower-roman)"."counter(d3, lower-latin)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l5> li:first-child>*:first-child:before {counter-increment: d3 0;  }
 #l6 {padding-left: 0pt;counter-reset: e1 3; }
 #l6> li>*:first-child:before {counter-increment: e1; content: counter(e1, upper-roman)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l6> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l7 {padding-left: 0pt;counter-reset: e2 1; }
 #l7> li>*:first-child:before {counter-increment: e2; content: counter(e1, upper-roman)"."counter(e2, upper-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l7> li:first-child>*:first-child:before {counter-increment: e2 0;  }
 li {display: block; }
 #l8 {padding-left: 0pt;counter-reset: f1 4; }
 #l8> li>*:first-child:before {counter-increment: f1; content: counter(f1, upper-roman)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l8> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l9 {padding-left: 0pt;counter-reset: f2 1; }
 #l9> li>*:first-child:before {counter-increment: f2; content: counter(f1, upper-roman)"."counter(f2, upper-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l9> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 li {display: block; }
 #l10 {padding-left: 0pt;counter-reset: g1 1; }
 #l10> li>*:first-child:before {counter-increment: g1; content: counter(g1, upper-roman)". "; color: #292929; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l10> li:first-child>*:first-child:before {counter-increment: g1 0;  }
 #l11 {padding-left: 0pt;counter-reset: g2 1; }
 #l11> li>*:first-child:before {counter-increment: g2; content: counter(g1, upper-roman)"."counter(g2, lower-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l11> li:first-child>*:first-child:before {counter-increment: g2 0;  }
 #l12 {padding-left: 0pt;counter-reset: g2 1; }
 #l12> li>*:first-child:before {counter-increment: g2; content: counter(g1, upper-roman)"."counter(g2, lower-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l12> li:first-child>*:first-child:before {counter-increment: g2 0;  }
 #l13 {padding-left: 0pt; }
 #l13> li>*:first-child:before {content: "• "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l14 {padding-left: 0pt;counter-reset: h1 1; }
 #l14> li>*:first-child:before {counter-increment: h1; content: counter(h1, decimal)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l14> li:first-child>*:first-child:before {counter-increment: h1 0;  }
 #l15 {padding-left: 0pt; }
 #l15> li>*:first-child:before {content: "• "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; }
 #l16 {padding-left: 0pt;counter-reset: j1 2; }
 #l16> li>*:first-child:before {counter-increment: j1; content: counter(j1, upper-roman)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l16> li:first-child>*:first-child:before {counter-increment: j1 0;  }
 #l17 {padding-left: 0pt;counter-reset: j2 4; }
 #l17> li>*:first-child:before {counter-increment: j2; content: counter(j1, upper-roman)"."counter(j2, lower-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l17> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l18 {padding-left: 0pt;counter-reset: j3 1; }
 #l18> li>*:first-child:before {counter-increment: j3; content: counter(j1, upper-roman)"."counter(j2, lower-roman)"."counter(j3, lower-latin)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l18> li:first-child>*:first-child:before {counter-increment: j3 0;  }
 #l19 {padding-left: 0pt;counter-reset: k1 3; }
 #l19> li>*:first-child:before {counter-increment: k1; content: counter(k1, upper-roman)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l19> li:first-child>*:first-child:before {counter-increment: k1 0;  }
 #l20 {padding-left: 0pt;counter-reset: k2 1; }
 #l20> li>*:first-child:before {counter-increment: k2; content: counter(k1, upper-roman)"."counter(k2, upper-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l20> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l21 {padding-left: 0pt;counter-reset: l1 1; }
 #l21> li>*:first-child:before {counter-increment: l1; content: counter(l1, lower-latin)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l21> li:first-child>*:first-child:before {counter-increment: l1 0;  }
 #l22 {padding-left: 0pt;counter-reset: m1 4; }
 #l22> li>*:first-child:before {counter-increment: m1; content: counter(m1, upper-roman)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l22> li:first-child>*:first-child:before {counter-increment: m1 0;  }
 #l23 {padding-left: 0pt;counter-reset: m2 1; }
 #l23> li>*:first-child:before {counter-increment: m2; content: counter(m1, upper-roman)"."counter(m2, upper-roman)". "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l23> li:first-child>*:first-child:before {counter-increment: m2 0;  }
 li {display: block; }
 #l24 {padding-left: 0pt;counter-reset: n1 1; }
 #l24> li>*:first-child:before {counter-increment: n1; content: "["counter(n1, decimal)"] "; color: black; font-family:"Courier New", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l24> li:first-child>*:first-child:before {counter-increment: n1 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><h1 style="padding-top: 4pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">BAŞKENT UNIVERSITY</h1><h1 style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">ENGINEERING FACULTY</h1><h1 style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">DEPARTMENT OF ELECTRICAL-ELECTRONICS ENGINEERING</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 44pt;text-indent: 0pt;line-height: 113%;text-align: center;">A COMPREHENSIVE REVIEW OF FACE DETECTION, TRACKING &amp; FACE SWAP ALGORITHMS WITH MATLAB APPLICATIONS</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 44pt;text-indent: 0pt;text-align: center;">PROJECT REPORT</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 163pt;text-indent: 46pt;line-height: 229%;text-align: left;">SAMET BAYAT ANKARA – DECEMBER 2022</h1><h3 style="padding-top: 3pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">ABSTRACT</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 22pt;text-indent: 0pt;text-align: left;">NAME - SURNAME: Samet Bayat</h3><h3 style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: left;">PROJECT TITLE : A COMPREHENSIVE REVIEW OF FACE DETECTION, TRACKING &amp; FACE SWAP WITH MATLAB APPLICATIONS</h3><h3 style="padding-left: 22pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Başkent University</h3><h3 style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Department of Electrical-Electronics Engineering</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">In this study face/feature detection, feature tracking and face swap applications will be handled and evaluated with Viola Jones, KLT and MPB algorithms. The work includes three different applications: &#39;Real Time Face Tracking&#39;, &#39;Image Editing/Blending&#39; and &#39;Video Editing/Blending&#39;. The open- source content of Mahmoud Afifi (York University) is used as the video processing dataset and MATLAB code guide. The aim is to examine the algorithms in detail, how MATLAB used it and to determine the proportionality of the application-theoretical knowledge common shares.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;">Keywords: Viola Jones, KLT, MPB, Face Swap, MATLAB.</p><h3 style="padding-top: 3pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">CONTENTS</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li data-list-text="I."><p style="padding-left: 40pt;text-indent: -18pt;text-align: left;"><a href="#bookmark4" class="s1">INTRODUCTION                          4</a></p><ol id="l2"><li data-list-text="I.i."><p style="padding-top: 5pt;padding-left: 88pt;text-indent: -30pt;text-align: left;"><a href="#bookmark5" class="s1">SOME FACE MANIPULATION METHODS</a></p></li></ol></li><li data-list-text="II."><p style="padding-top: 22pt;padding-left: 46pt;text-indent: -24pt;text-align: left;"><a href="#bookmark8" class="s1">BEHIND THE SCENES: ALGORITHMS &amp; MATH BEHIND APPLICATIONS     5</a></p><p style="padding-top: 5pt;padding-left: 58pt;text-indent: 0pt;text-align: left;"><a href="#bookmark9" class="s1">II.i – VIOLA JONES ALGORITHM (FOR FACE AND FEATURE DETECTION)</a></p><p style="padding-top: 5pt;padding-left: 58pt;text-indent: 0pt;text-align: left;"><a href="#bookmark18" class="s1">II.II KANADE–LUCAS–TOMASI FEATURE TRACKER</a></p><p style="padding-top: 5pt;padding-left: 58pt;text-indent: 0pt;text-align: left;"><a href="#bookmark19" class="s1">II.III. IMAGE PREPROCESSING (IMAGE ALIGNMENT &amp; STABILIZATION)</a></p><ol id="l3"><ol id="l4"><li data-list-text="II.iv."><p style="padding-top: 5pt;padding-left: 100pt;text-indent: -42pt;text-align: left;"><a href="#bookmark20" class="s1">MODIFIED POISSON BLENDING TECNIQUE (MPB)</a></p><ol id="l5"><li data-list-text="II.iv.a."><p style="padding-top: 7pt;padding-left: 148pt;text-indent: -54pt;text-align: left;"><a href="#bookmark1" class="s1" name="bookmark0">WHY ‘MODIFIED’?</a><a name="bookmark1">&zwnj;</a></p></li><li data-list-text="II.iv.b."><p style="padding-top: 1pt;padding-left: 148pt;text-indent: -54pt;text-align: left;"><a href="#bookmark21" class="s1">WHAT IS ‘POISSON’ DISTRIBUTION?</a></p></li><li data-list-text="II.iv.c."><p style="padding-top: 5pt;padding-left: 148pt;text-indent: -54pt;text-align: left;"><a href="#bookmark22" class="s1">HOW ‘BLENDING’ WORKS?</a></p></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="III."><p style="padding-left: 52pt;text-indent: -30pt;text-align: left;"><a href="#bookmark27" class="s1">IMAGE &amp; VIDEO FACE SWAP APPLICATIONS IN MATLAB          20</a></p><ol id="l6"><ol id="l7"><li data-list-text="III.I."><p style="padding-top: 5pt;padding-left: 100pt;text-indent: -42pt;text-align: left;"><a href="#bookmark28" class="s1">REAL-TIME FACE DETECTION AND TRACKING</a></p></li><li data-list-text="III.II."><p style="padding-top: 5pt;padding-left: 106pt;text-indent: -48pt;text-align: left;"><a href="#bookmark29" class="s1">POISSON IMAGE EDITING (FACE&amp;OBJECT SWAP / IMAGE)</a></p></li><li data-list-text="III.III."><p style="padding-top: 5pt;padding-left: 112pt;text-indent: -54pt;text-align: left;"><a href="#bookmark30" class="s1">VIDEO FACE SWAP APPLICATION</a></p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="IV."><p style="padding-left: 46pt;text-indent: -24pt;text-align: left;"><a href="#bookmark35" class="s1">EXEMPLARY APPLICATIONS                     23</a></p></li></ol><ol id="l8"><ol id="l9"><li data-list-text="IV.I."><p style="padding-top: 5pt;padding-left: 94pt;text-indent: -36pt;text-align: left;"><a href="#bookmark36" class="s1">HIGH-RESOLUTION NEURAL FACE SWAPPING FOR VISUAL EFFECTS</a></p></li><li data-list-text="IV.II."><p style="padding-top: 5pt;padding-left: 100pt;text-indent: -42pt;text-align: left;"><a href="#bookmark37" class="s1">SYNTHETIC FILM DUBBING</a></p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:19.525pt" cellspacing="0"><tr style="height:23pt"><td style="width:138pt"><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark38" class="s2">IV.III. DEEPFAKE</a></p></td><td style="width:168pt"><p style="padding-left: 3pt;text-indent: 0pt;text-align: left;"><a href="#bookmark38" class="s2">ANCHOR</a></p></td><td style="width:143pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:34pt"><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark40" class="s2">V. ETHICAL ISSUES</a></p></td><td style="width:168pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:143pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark40" class="s2">24</a></p></td></tr><tr style="height:34pt"><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark42" class="s2">VI. CONCLUSION</a></p></td><td style="width:168pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:143pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark42" class="s2">24</a></p></td></tr><tr style="height:34pt"><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark44" class="s2">REFERENCES</a></p></td><td style="width:168pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:143pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark44" class="s2">25</a></p></td></tr><tr style="height:23pt"><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark46" class="s2">FIGURE REFERENCES</a></p></td><td style="width:168pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:143pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-right: 2pt;text-indent: 0pt;line-height: 10pt;text-align: right;"><a href="#bookmark46" class="s2">26</a></p></td></tr></table><p style="padding-top: 3pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Masking and face manipulation is nowadays’ vouge. Technology and algorithms behind it evaluated day by day, by dint of this improvement, we use these applications more common. In this paper, the process behind of that explained in detail. There are mainly 3 approaches/algorithms (Viola-Jones, KLT, MBP) used to editing/manipulating images and videos. Topics handled in 6 parts. In first part, the differences between the place of mask in human culture and the practices, in 2<span class="s3">nd  </span>part, BEHIND THE SCENES: ALGORITHMS AND MATH BEHIND APPLICATIONS, in 3<span class="s3">rd </span>part IMAGE &amp; VIDEO FACE SWAP APPLICATIONS IN MATLAB, in 4<span class="s3">th </span>part EXEMPLARY APPLICATIONS IN THE LITERATURE, in 5<span class="s3">th </span>part, ETHICAL ISSUES and</p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">in last part results were evaluated are discussed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l10"><li data-list-text="I."><p class="s4" style="padding-left: 40pt;text-indent: -18pt;text-align: left;"><a name="bookmark2">INTRODUCTION</a><a name="bookmark4">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="106" height="109" alt="A close-up of a person  Description automatically generated with low confidence" title="A close-up of a person  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_001.jpg"/></span></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">From the very beginning of life, humans try to express him/herself with ‘Masks’.[1] To surprise, to frighten, to pray, to cover... Like any other imperishable tradition, we use its modified version nowadays. Unlike when cinema discovered VFX in analog [2,3], the breakthrough and spread of digitalization have made interpretations doable-by-anyone, limitless and unpredictable. Moreover, ‘digital masks/filters’ became vouge of our virtual world and even affect in our attitudes in non-virtual areas. [4,5] In my work, I examined the reflection of this socio-cultural fact and trend on the computer age with some algorithms and case studies as image/video face swap and filtering applications which was made using Viola Jones Algorithm, KLT (Kanade–Lucas–Tomasi feature tracker) and MPB (Modified Poisson blending technique).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 111pt;text-indent: 0pt;line-height: 114%;text-align: left;">Figure 1. Le Voyage</p><p style="padding-left: 111pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Dans La Lune,</p><p style="padding-top: 1pt;padding-left: 111pt;text-indent: 0pt;text-align: left;">A Trip to The Moon (1902)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l11"><li data-list-text="I.i."><h3 style="padding-left: 52pt;text-indent: -30pt;text-align: left;"><a name="bookmark3">SOME FACE MANIPULATION METHODS</a><a name="bookmark5">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Face manipulation concept is so wide, in literature there are many different approaches to achieve this problem. In common, the blending process progresses with the mask which taken from source applies to target’s face. Unlike Face2Face (3D masks), in our concept ‘Face Swap’, we use 2-dimensional masks (projections). On the other hand, <i>deepfake </i>refers to more complex applications. In deepfake approaches, generally GAN or Autoencoder-like deep learning applications are used. [6,7,8]</p></li></ol></li><li data-list-text="II."><p class="s4" style="padding-top: 4pt;padding-left: 46pt;text-indent: -24pt;text-align: left;"><a name="bookmark6">BEHIND THE SCENES: ALGORITHMS &amp; MATH BEHIND APPLICATIONS</a><a name="bookmark8">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">In this section algorithms and some related application outputs will be discussed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l12"><li data-list-text="II.i."><h3 style="padding-left: 52pt;text-indent: -30pt;text-align: left;"><a name="bookmark7">– VIOLA JONES ALGORITHM (FOR FACE AND FEATURE DETECTION)</a><a name="bookmark9">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Despite its introduction at the beginning of the millennium, Viola Jones Algorithm is still one of the most popular face detection algorithms. It classifies values of images based on the simple ‘features’ rather than pixels. The main reason to do that is feature-based system operates much faster than a pixel-based system. At this point <i>Haar basis functions </i>and <i>Integral Image </i>help us to detect face(s) and simple features, for this process we use three kinds of Haar-like features: [9,10]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l13"><li data-list-text="•"><p style="padding-left: 58pt;text-indent: -18pt;text-align: left;">Edge-like-features (two-rectangle-features)     <span><img width="37" height="20" alt="image" src="Report_Samet_Bayat_22293730_final_files/Image_002.jpg"/></span><span class="s6"> </span>(6 memory lookups)</p><p class="s7" style="padding-top: 2pt;padding-left: 58pt;text-indent: 0pt;line-height: 113%;text-align: justify;">The value of a two-rectangle feature is the difference between the sum of the pixels within two rectangular regions. The regions have the same size and shape and are horizontally or vertically adjacent.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="•"><p style="padding-left: 58pt;text-indent: -18pt;line-height: 110%;text-align: left;">Line-like-features (three-rectangle-features)     <span><img width="40" height="22" alt="A picture containing text, picture frame, window  Description automatically generated" title="A picture containing text, picture frame, window  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_003.jpg"/></span><span class="s8"> </span>(8 memory lookups)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="37" height="22" alt="A picture containing table  Description automatically generated" title="A picture containing table  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_004.jpg"/></span></p><p class="s7" style="padding-left: 58pt;text-indent: 0pt;line-height: 115%;text-align: justify;">A three-rectangle feature computes the sum within two outside rectangles subtracted from the sum in a center rectangle</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="•"><p style="padding-left: 58pt;text-indent: -18pt;line-height: 113%;text-align: left;">Four-rectangle-features / Diagonal-features (9 memory lookups)</p><p class="s7" style="padding-left: 58pt;text-indent: 0pt;text-align: left;">Four-rectangle feature computes the difference between diagonal pairs of rectangles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 58pt;text-indent: 0pt;text-align: left;">Edge-like  2 Rectangle <span class="s9">= A-2B+C-D+2E-F</span></h4><h4 style="padding-top: 1pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">Line-like  3 Rectangle <span class="s9">= A-B-2C+2D+2E-2F-G+H</span></h4><h4 style="padding-top: 1pt;padding-bottom: 4pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">Diagonal   4 Rectangle <span class="s9">= A-2B+C-2D+4E-2F+H-2I+J</span></h4><p style="padding-left: 132pt;text-indent: 0pt;text-align: left;"><span><img width="371" height="101" alt="Chart, box and whisker chart  Description automatically generated" title="Chart, box and whisker chart  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_005.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 2. Feature Lookups</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The process that makes this face recognition algorithm faster than others is <i>Integral Image</i>. It allows to compute related part of image much faster with less variables compared to pixel-to-pixel computation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The integral image at location x, y contains the sum of the pixels above and to the left of x, y, inclusive:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 201pt;text-indent: 0pt;text-align: left;"><span><img width="143" height="30" alt="A picture containing shape  Description automatically generated" title="A picture containing shape  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_006.jpg"/></span></p><p style="padding-top: 3pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">where ii(x, y) is the integral image and i(x, y) is the original image. Using the following pair of recurrences:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 147pt;text-indent: 0pt;text-align: left;"><span><img width="288" height="38" alt="A close-up of a calculator  Description automatically generated with low confidence" title="A close-up of a calculator  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_007.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;">(where s(x, y) is the cumulative row sum, s(x, −1) = 0, and ii(−1, y) = 0) the</p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">integral image can be computed in one pass over the original image. [9]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Feature Discussion: <span class="p">It is obvious that these filters are not the solutions that give the best results for face detection and feature extraction. Especially for detailed analysis. Rectangle features are somewhat primitive when compared with alternatives such as steerable ﬁlters [11] Contrastingly, it could be said that horizontal, vertical, and diagonal lines are the most optimal solution to determine the borders and features of face. An example for feature detection with integral image shown below:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 146pt;text-indent: 0pt;text-align: left;"><span><img width="304" height="198" alt="A picture containing text, crossword puzzle  Description automatically generated" title="A picture containing text, crossword puzzle  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_008.jpg"/></span></p><p style="padding-top: 8pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Figure 3. On the left, desired part of original image is sum of 9-pixel values. On the right-hand-side, the same desired area calculated with only 4 variables.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;">For our case, MATLAB offers different kind of solutions.</p><ol id="l14"><li data-list-text="1."><p style="padding-top: 1pt;padding-left: 58pt;text-indent: -18pt;text-align: left;">Related images’ pixel values can be transferred into matrix and the</p><p style="padding-top: 1pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">calculation that mentioned above apply to the matrix.</p></li><li data-list-text="2."><p style="padding-top: 1pt;padding-left: 58pt;text-indent: -18pt;line-height: 114%;text-align: left;">MATLAB built-in functions allows us to do reach <i>solution 1 </i>with less line of code. [12]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="71" height="85" alt="A person wearing a hat  Description automatically generated with low confidence" title="A person wearing a hat  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_009.jpg"/></span></p><p class="s7" style="padding-left: 94pt;text-indent: 0pt;line-height: 115%;text-align: left;">I = imread(&#39;pout.tif&#39;); imshow(I)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 5pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">J = integralImage(I);</p><p class="s7" style="padding-top: 1pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">d = drawrectangle;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="73" height="88" alt="Graphical user interface, application  Description automatically generated" title="Graphical user interface, application  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_010.jpg"/></span></p><p class="s7" style="padding-left: 315pt;text-indent: 0pt;text-align: left;">Figure 4. ‘pout.tif’</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 94pt;text-indent: 0pt;line-height: 112%;text-align: left;">r = floor(d.Vertices(:,2)) + 1; c = floor(d.Vertices(:,1)) + 1;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">regionSum = J(r(1),c(1)) - J(r(2),c(2)) ...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 5pt;padding-left: 271pt;text-indent: 0pt;text-align: left;">Figure 5. Area where calculations made</p><p class="s7" style="padding-top: 4pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">+ J(r(3),c(3)) - J(r(4),c(4))</p><p class="s11" style="padding-top: 4pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">regionSum = <b>1129512</b></p></li><li data-list-text="3."><p style="padding-top: 4pt;padding-left: 58pt;text-indent: -18pt;line-height: 114%;text-align: justify;">The most proper and fastest solution for this particular case is <i>vision.CascadeObjectDetector()</i>. This 1 line of code not just provides the integral image, it nearly includes all the methods discussed above and will be discuss below. (Haar-basis functions, cascading etc.) [13]</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><a name="bookmark10">Adaptive Boosting (AdaBoost) Algorithm</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The process which combines several weak learners (for our case, weak learner refers to each facial organs: nose, left eye, mouth, etc.) into a strong learner called boosting. (This process could be considered as complementary of Cascading) Key point here is training period lasts sequentially. Which means the purpose of every single step is to correct its predecessor feature. One of the major concerns of Adaptive Boosting is under-fitting situation. In this instance, model does not fit the training data, so the basic relationship between inputs and outputs cannot be learned. The error rate is high in both the training and test set. These models miss trends in the data and cannot generalize. To avoid this problem, first base classifiers (ex. Decision Tree, Support Vector Machines...) often used. Since the AdaBoost Algorithm is generally used for binary classification, its main duty is to improve calculation of the distinction between faces and non-faces. (To achieve this aim like in ‘Decision Tree’, it uses weights(w) between sequences, branches and tries to find the minimal error rates) As mentioned above AdaBoost should not be treated as ‘face’ detector. (It improves the results) We can call the concept that detects the face ‘Cascading’. Finally, a new predictor is trained using the updated weights, and the whole process is repeated until the desired number of predictors is reached which is specified by the user. [14]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="100" height="34" alt="Shape  Description automatically generated with medium confidence" title="Shape  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_011.png"/></span></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: justify;"><span><img width="87" height="58" alt="Shape  Description automatically generated with medium confidence" title="Shape  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_012.png"/></span><span class="s15"> </span>,      ,<span><img width="210" height="44" alt="Shape  Description automatically generated with medium confidence" title="Shape  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_013.png"/></span>,<span><img width="121" height="47" alt="Shape  Description automatically generated with medium confidence" title="Shape  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_014.png"/></span></p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 6. AdaBoost Algorithm Calculations (Briefly)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 22pt;text-indent: 0pt;text-align: justify;"><a name="bookmark11">Cascade Filter</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">After we found the rough position of face, cascade filter (it could be considered as binary classifier) searches a particular niche feature (an eye, ear, mouth, etc.) During search, if it detects a particular feature (ex. Right eye) it continues to search another feature. (ex. nose) If it gets always positive responses (finds desired features): finally, it returns the ‘ROI’ (Region of Interest) of face with locations of features. If it cannot find desired feature: it stops the process (Reduces the amount of computation time spent on false, windows) retries the process on different area of image.</p><p style="padding-left: 162pt;text-indent: 0pt;text-align: left;"><span><img width="249" height="114" alt="https://levelup.gitconnected.com/artificial-intelligence-how-the-viola-jones-algorithm-help-in-object-detection-28320596a81c " title="https://levelup.gitconnected.com/artificial-intelligence-how-the-viola-jones-algorithm-help-in-object-detection-28320596a81c " src="Report_Samet_Bayat_22293730_final_files/Image_015.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 7. Cascade Classifier</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">There are some results of my study shared below. It takes an average of 0.1 seconds for the machine to detect that there is no head in the frame. Depending on the number of heads in the image, the finding time varies between <span style=" color: #000;">0.164459 and 1.207437 seconds. </span>These results confirm that when non-face area (false positives) in image decreases, machine detection rate increases. Since the machine was running unsupervised (we show that what head is look like and what is not to the machine yet, it was not possible to show every condition that is non-head) therefore it was not possible to create a proper confusion matrix, so the situation is explained with the following case examples. If we examine the performances and failures of the model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 22pt;text-indent: 0pt;text-align: justify;">Dataset For Feature Training</p><p class="s14" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">At the very beginning of coding, I try to evaluate feature training stage with external dataset. In my trials, I tried to use ‘MS Celeb 1M’ [15] dataset which contains 8,456,240 images yet due to some restrictions of operating system and graphics card (MacOSX – M1 GPU)* the process could not be completed with desired conditions. Therefore, I used ‘Transfer Learning’ concept for feature detection phase, from OpenCV libraries, I transfer ‘<b>haarcascade_frontalface_alt.xml</b>’ file that includes feature detection information about frontal face. [16,17] Here machine evaluates that information with using Haar-like Features and cascading. (these principals discussed above.) In addition to this, I also use built in MATLAB Haar-Cascade module called ‘FrontalFaceCART’.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 22pt;text-indent: 0pt;text-align: justify;">Accuracy of model:</p><p class="s14" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">During trials, machine found 175 out of 197 potential heads, and 9 non-heads were classified as head. (Since we work with single-head models for <u>face swapping</u> applications, the margin of error for multi-head models is negligible.)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-left: 22pt;text-indent: 0pt;text-align: justify;">Accuracy = 0.8495145631 ≈ 84.95%</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 22pt;text-indent: 0pt;text-align: justify;">Average finding time:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">0.164459 (for 1 face)</p><p class="s14" style="padding-top: 1pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">= 0.164459 sec / face</p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">0.423240 <span style=" color: #292929;">(for 29 face, it also classifies 6 non-head situations as face)</span></p><p class="s14" style="padding-top: 1pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">= 0.014594 sec / face</p><p class="s14" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">1.207437 (for 133 face, machine also couldn’t find 2 of faces)</p><p class="s14" style="padding-top: 1pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">= 0.009078 sec / face</p><p class="s7" style="padding-top: 4pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 176pt;text-indent: 0pt;line-height: 114%;text-align: center;">Elapsed time is 0.109801 seconds. Elapsed time is 0.087844 seconds. Elapsed time is 0.198364 seconds. &#39; No face detected : ( &#39;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 176pt;text-indent: 0pt;line-height: 114%;text-align: center;">Elapsed time is 0.423240 seconds. Elapsed time is 1.207437 seconds. Elapsed time is 0.164459 seconds. &#39; Face detected : ) &#39;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 44pt;text-indent: 0pt;text-align: center;">...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span><img width="341" height="207" alt="A screenshot of a cell phone  Description automatically generated with low confidence" title="A screenshot of a cell phone  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_016.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 8.</p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Elapsed time is 1.207437 seconds.</p><p style="padding-top: 1pt;padding-left: 187pt;text-indent: 0pt;text-align: left;">133 Face(s) Detected :)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Machine could not distinguish faces in photos taken from side profile. The training dataset may be lack of or includes not enough number of photos which taken from side-profiles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="136" alt="image" src="Report_Samet_Bayat_22293730_final_files/Image_017.png"/></span></p><p class="s16" style="padding-top: 5pt;padding-left: 29pt;text-indent: 0pt;line-height: 9pt;text-align: left;">case <span style=" color: #000;">0</span></p><p class="s7" style="padding-left: 29pt;text-indent: 19pt;text-align: left;">colist{ii} = [<span style=" color: #A709F5;">&#39;red&#39;</span>]; <span style=" color: #0D00FF;">case </span>1</p><p class="s7" style="padding-left: 29pt;text-indent: 19pt;text-align: left;">colist{ii} = [<span style=" color: #A709F5;">&#39;green&#39;</span>]; <span style=" color: #0D00FF;">case </span>2</p><p class="s7" style="padding-left: 29pt;text-indent: 19pt;text-align: left;">colist{ii} = [<span style=" color: #A709F5;">&#39;cyan&#39;</span>]; <span style=" color: #0D00FF;">case </span>3</p><p class="s7" style="padding-left: 29pt;text-indent: 19pt;text-align: left;">colist{ii} = [<span style=" color: #A709F5;">&#39;magenta&#39;</span>]; <span style=" color: #0D00FF;">case </span>4</p><p class="s7" style="padding-left: 29pt;text-indent: 19pt;text-align: left;">colist{ii} = [<span style=" color: #A709F5;">&#39;blue&#39;</span>]; <span style=" color: #0D00FF;">otherwise</span></p><p class="s7" style="padding-left: 48pt;text-indent: 0pt;line-height: 9pt;text-align: left;">colist{ii} = [<span style=" color: #A709F5;">&#39;black&#39;</span>];</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 13pt;text-indent: 0pt;line-height: 114%;text-align: justify;">On the left is a block of face detection code. This simple iteration is not added just for visual concerns. The conclusion we will draw by following these colors is this: the machine does not follow the rows and columns sequentially (at least as I predicted at the beginning).</p><p style="padding-left: 141pt;text-indent: 0pt;text-align: left;"><span><img width="304" height="216" alt="A picture containing text, person  Description automatically generated" title="A picture containing text, person  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_018.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 157pt;text-indent: 3pt;line-height: 114%;text-align: left;">Figure 9. Сталкер(Stalker),1979. Elapsed time is 0.229140 seconds.</p><p style="padding-left: 193pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1 Face(s) Detected :)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Machine could not distinguish face in photo in ‘sepia’ format with low lighting. <span class="p">One of the possibilities behind that failure might be the default ‘threshold’ value. In the work below, we can observe how threshold effects the result:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 34pt;text-indent: 0pt;line-height: 114%;text-align: justify;">OrganDetect_a = vision.CascadeObjectDetector(<span style=" color: #A709F5;">&#39;Nose&#39;</span>, <span style=" color: #A709F5;">&#39;MergeThreshold&#39;</span>, 2 ) OrganDetect_b = vision.CascadeObjectDetector(<span style=" color: #A709F5;">&#39;Nose&#39;</span>, <span style=" color: #A709F5;">&#39;MergeThreshold&#39;</span>, 12) OrganDetect_c = vision.CascadeObjectDetector(<span style=" color: #A709F5;">&#39;Nose&#39;</span>, <span style=" color: #A709F5;">&#39;MergeThreshold&#39;</span>, 22)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;text-align: left;"><span><img width="424" height="168" alt="image" src="Report_Samet_Bayat_22293730_final_files/Image_019.png"/></span></p><p style="padding-top: 1pt;padding-left: 226pt;text-indent: 0pt;text-align: left;">Figure 10.</p><p style="padding-top: 1pt;padding-left: 154pt;text-indent: 0pt;text-align: left;">(a) Threshold:2, (b) Threshold:12,</p><p style="padding-top: 1pt;padding-left: 205pt;text-indent: 0pt;text-align: left;">(c) Threshold:22,</p><p style="padding-top: 1pt;padding-left: 154pt;text-indent: 0pt;text-align: left;">(Face model: Alejandro Jodorovsky)</p><p style="padding-left: 111pt;text-indent: 0pt;text-align: left;"><span><img width="384" height="284" alt="A picture containing outdoor, group, line, bunch  Description automatically generated" title="A picture containing outdoor, group, line, bunch  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_020.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 11.</p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Elapsed time is 0.80642 seconds.</p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">35 Face(s) Detected :)</p><p class="s5" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Machine detected 6 non-face as face.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 116pt;text-indent: 0pt;text-align: left;"><span><img width="371" height="201" alt="A person in a suit and tie  Description automatically generated with medium confidence" title="A person in a suit and tie  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_021.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 12. The Wolf of Wall Street, 2013</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">This study was performed only under experimental concerns. This example was given to the machine before the face features was described, and as expected, the machine failed. In crowded environments, the detect rate has been improved as shared in the examples above.</p><p style="padding-left: 149pt;text-indent: 0pt;text-align: left;"><span><img width="284" height="179" alt="A person with green eyes  Description automatically generated with low confidence" title="A person with green eyes  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_022.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 13. The Office, 2005</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 29pt;text-indent: 0pt;text-align: center;">In this example, face detected successfully yet, not enough features found.</p><p class="s5" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Especially, mouth could not detected.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 158pt;text-indent: 0pt;text-align: left;"><span><img width="259" height="236" alt="image" src="Report_Samet_Bayat_22293730_final_files/Image_023.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 14. Truman Show, 1998</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: left;">Here machine detect all features, but it cannot find enough arguments for nose and eyes. It also accepts turtleneck sweater as face feature.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 176pt;text-indent: 0pt;text-align: left;"><span><img width="211" height="180" alt="A person with green paint on his face  Description automatically generated with medium confidence" title="A person with green paint on his face  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_024.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 15.</p><p class="s5" style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;line-height: 114%;text-align: center;">This example can be considered as successful trial. Face and features detected as expected and we observe enough arguments for each feature.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-left: 44pt;text-indent: 0pt;text-align: center;">NOTE: Machine gives much better results in people who do not have or have less hair.</p><p style="padding-left: 207pt;text-indent: 0pt;text-align: left;"><span><img width="113" height="138" alt="Papa Smurf" title="Papa Smurf" src="Report_Samet_Bayat_22293730_final_files/Image_025.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 16. The Smurfs, 1981.</p><p class="s5" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Additionally, machine classified Papa Smurf as non-face.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Before face swap applications, a basic application tried with these feature locations. After bunch of trial-error period, simple glasses filter added to target face. The promising result here is alpha composition (see MBP part) of source (glasses transparency) is preserved. (In other words, we still can see targets’ eyes)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 132pt;text-indent: 0pt;text-align: left;"><span><img width="329" height="135" alt="image" src="Report_Samet_Bayat_22293730_final_files/Image_026.png"/></span></p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 17. Simple eyeglasses application example.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><a name="bookmark12">II.II KANADE–LUCAS–TOMASI FEATURE TRACKER</a><a name="bookmark18">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 117%;text-align: left;">KLT Algorithm is capable of track objects with many methods, varieties, different motions: [18]</p><ul id="l15"><li data-list-text="•"><p style="padding-left: 34pt;text-indent: -12pt;line-height: 11pt;text-align: left;">Tracking deals with estimating the trajectory</p><p style="padding-top: 1pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">of an object in the image plane as it moves around a scene.</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Object tracking (car, airplane, person)</p></li><li data-list-text="•"><h3 style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Feature (Harris corners) Tracking</h3></li><li data-list-text="•"><h3 style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Single object tracking</h3></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Multiple Object tracking</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Tracking in fixed camera</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Tracking in moving camera</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Tracking in multiple cameras</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Translation</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Euclidean</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Similarity</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Affine</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">Projective</p><p style="padding-top: 3pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Each of them needs different calculations to reach most accurate solution. From the first release of KLT Algorithm (1981), it changed and expanded majorly 2 times (1984 and 1991):</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="•"><h3 style="padding-left: 22pt;text-indent: 0pt;line-height: 117%;text-align: left;">Lucas-Kanade (1981): <span class="p">An Iterative Image Registration Technique with an Application to Stereo Vision.[19]</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="•"><h3 style="padding-left: 34pt;text-indent: -12pt;text-align: left;">Kanade-Tomasi (1991): <span class="p">Detection and Tracking of Feature Points.[20]</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="•"><h3 style="padding-left: 34pt;text-indent: -12pt;text-align: left;">Tomasi-Shi (1994): <span class="p">Good Features to Track.[21]</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The  overall  idea  of  the  algorithm  answers  and  creates  two  key questions/dilemmas:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Q1. How should we track them from frame to frame?</p><p style="padding-top: 1pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">A1. Method for aligning (tracking) an image patch (1981)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Q2. How should we select features?</p><p style="padding-top: 1pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">A2. Method for choosing the best feature (image patch) for tracking. (1991)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">For our case, we have to detect and track face parts carefully and as fast as we could. At this point, what we need is to detect ‘good features’ which can be tracked easier and more precisely compared to the other features. Like Haar- basis-edge-like detection, first we need a region that contains only the related part of tracked face. Therefore, initially we generate a ROI (Region of interest) from photo and ROI can also be considered as a roadmap of our process. (It includes eyes’, nose’s, mouth’s, chin’s, foreheads, and eyebrows’ location which are our good features to track.) After that, variety of calculations show themselves up. Some of basic calculations given in below:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 150pt;text-indent: 0pt;text-align: left;"><span><img width="281" height="157" alt="PERSON OF INTEREST 2.01 - THE CONTINGENCY" title="PERSON OF INTEREST 2.01 - THE CONTINGENCY" src="Report_Samet_Bayat_22293730_final_files/Image_027.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 18. Person of Interest, 2011</p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Yellow square represents Region of Interest (ROI).</p><p style="padding-left: 71pt;text-indent: 0pt;text-align: left;"><span><img width="484" height="160" alt="Shape, polygon  Description automatically generated" title="Shape, polygon  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_028.jpg"/></span></p><p style="padding-top: 5pt;padding-left: 166pt;text-indent: 0pt;line-height: 114%;text-align: center;">Figure 19. Movement Situations source: S. Cheng, 2018</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 85pt;text-indent: 0pt;text-align: left;"><span><img width="452" height="222" alt="A screenshot of a computer  Description automatically generated with low confidence" title="A screenshot of a computer  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_029.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 133pt;text-indent: 0pt;line-height: 114%;text-align: center;">Figure 20. Basis of Tracking Equations source: S. Cheng, 2018</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: justify;">From now on, we have two options for tracking:</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="•"><p style="padding-left: 22pt;text-indent: 0pt;line-height: 117%;text-align: left;">Calculate every good feature movement (Enormous computational cost for large/long videos)</p></li><li data-list-text="•"><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: left;">Calculate Harris corners’ movement (Less accurate but very effective compared to the previous option.)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Harris Corner Detection <span class="p">allows us to detect corners in image. [That corner information (locations) is so useful for tracking and alignment phases] Harris detector evaluates eigenvalues to find the &#39;ROI&#39;. The process simply explained with figures below: [22]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 182pt;text-indent: 0pt;text-align: left;"><span><img width="245" height="120" alt="A picture containing text, clock  Description automatically generated" title="A picture containing text, clock  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_030.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 172pt;text-indent: 0pt;text-align: left;">Figure 21. Edge searching process.</p><p style="padding-left: 310pt;text-indent: 0pt;text-align: left;"><span><img width="192" height="89" alt="A picture containing toy, decorated, projector  Description automatically generated" title="A picture containing toy, decorated, projector  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_031.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="316" height="190" alt="Text  Description automatically generated" title="Text  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_032.jpg"/></span></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Figure 22. Steps of tracking process</p><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: center;">Figure 23. Corner Responses</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;"><span><img width="111" height="90" alt="1: Typical Harris corner detector response on the face" title="1: Typical Harris corner detector response on the face" src="Report_Samet_Bayat_22293730_final_files/Image_033.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 55pt;text-indent: 0pt;line-height: 114%;text-align: center;">Figure 24. Typical Harris corner detector response on the face</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The main difference between Harris detector-based feature tracking and ours (KLT Shi-Thomasi, 1994) is that our version uses just edges as good-features to track face. It obviously increases the error ratio but also speed up process significantly. Since we use these features in video processing, keeping the number of edges optimal will be much more efficient for the video editing process.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;">In MATLAB side, these main calculations done with three key functions:</p></li><li data-list-text="•"><h3 style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">points = detectMinEigenFeatures(rgb2gray(videoFrame), <span style=" color: #A709F5;">&#39;ROI&#39;</span>, bbox);</h3><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">This line finds the Harris corners from eigenvalues and uses ‘boundarybox’ (area</p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">separated from the rest of the image) for tracking. [23]</p></li><li data-list-text="•"><h3 style="padding-top: 1pt;padding-left: 34pt;text-indent: -12pt;text-align: left;">pointTracker = vision.PointTracker(<span style=" color: #A709F5;">&#39;MaxBidirectionalError&#39;</span>, 2);</h3><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: left;">Afterwards, we follow the detected points with the mathematical operations mentioned above. [24]</p></li><li data-list-text="•"><h3 style="padding-left: 94pt;text-indent: -72pt;text-align: left;">[xform, oldInliers, visiblePoints] = estimateGeometricTransform(<span style=" color: #0D00FF;">... </span>oldInliers, visiblePoints, <span style=" color: #A709F5;">&#39;similarity&#39;</span>, <span style=" color: #A709F5;">&#39;MaxDistance&#39;</span>, 4);</h3></li></ul><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: left;">‘estimateGeometricTransform’ function estimates the most suitable equations for tracking points with the help of equations in Figure 20.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">As the method used instead of this function in the first trials could not modify/predict the linear or polynomial equation obtained from the follow-up in the first two frames, the results were not as desired.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="354" height="154" alt="A person in a blue shirt  Description automatically generated with low confidence" title="A person in a blue shirt  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_034.jpg"/></span></p><h2 style="text-indent: 0pt;text-align: left;">0</h2><p style="text-indent: 0pt;text-align: left;"/><h2 style="text-indent: 0pt;text-align: left;">1</h2><p style="text-indent: 0pt;text-align: left;"/><h2 style="text-indent: 0pt;text-align: left;">9</h2><p style="text-indent: 0pt;text-align: left;"/><h2 style="text-indent: 0pt;text-align: left;">21</h2><p style="text-indent: 0pt;text-align: left;"/><h2 style="text-indent: 0pt;text-align: left;">17</h2><p style="text-indent: 0pt;text-align: left;"/><h2 style="text-indent: 0pt;text-align: left;">13</h2><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 296pt;text-indent: 0pt;text-align: left;">Figure 25.</p><p class="s21" style="padding-top: 1pt;padding-left: 296pt;text-indent: 0pt;line-height: 114%;text-align: left;"><span class="p">It </span>observed that (without using estimateGeometricTransform) after evaluation of 1<span class="s22">st </span>and 2<span class="s22">nd </span>frame, machine estimates a movement and uses it for each different frame without update. That causes a failure which can be seen from frame 9 to 21.</p><h3 style="padding-top: 3pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a name="bookmark13">II.III. IMAGE PREPROCESSING (IMAGE ALIGNMENT &amp; STABILIZATION)</a><a name="bookmark19">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Image alignment <span class="p">is the procedure of the overlay of the images of same scene under various conditions, such as from various viewpoints, with different illumination, using a variety of the sensors, or at various times. Image alignment is transforming a source image to the coordinate system of the reference image besides </span>image stabilization <span class="p">helps us to fix issues caused by blur (problems occur during exposure because of camera motion). [25]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">In these applications below, it showed that what happens if we do not use image alignment/stabilization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 172pt;text-indent: 0pt;text-align: left;"><span><img width="224" height="229" alt="A picture containing person, indoor, wall  Description automatically generated" title="A picture containing person, indoor, wall  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_035.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 26.a. The Office, 2005.</p><p class="s5" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Machine could not align target’s eyebrow perfectly but</p><p class="s5" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">in our case, it is a satisfied result.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 165pt;text-indent: 0pt;text-align: left;"><span><img width="242" height="251" alt="A person in a suit sitting at a table  Description automatically generated with medium confidence" title="A person in a suit sitting at a table  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_036.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 26.b. Evan Almighty, 2007.</p><p class="s5" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Completely unsuccessful example, except for the right eye</p><ol id="l16"><ol id="l17"><li data-list-text="II.iv."><h3 style="padding-top: 3pt;padding-left: 64pt;text-indent: -42pt;text-align: left;"><a name="bookmark14">MODIFIED POISSON BLENDING TECNIQUE (MPB)</a><a name="bookmark20">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The most relevant part of this work with image/video editing is definitely the MPB phase. Masking application which mentioned at the very beginning of the introduction takes its final shape with this technique. Although the verbal description is simple, computation behind is not. Because of that, MPB explanation covered under 3 sub-questions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l18"><li data-list-text="II.iv.a."><h3 style="padding-left: 76pt;text-indent: -54pt;text-align: left;"><a name="bookmark15">WHY ‘MODIFIED’?</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 160pt;text-indent: 0pt;text-align: left;"><span><img width="259" height="102" alt="A group of men  Description automatically generated with low confidence" title="A group of men  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_037.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 124pt;text-indent: 6pt;line-height: 114%;text-align: left;">Figure 27. Sample output of blending using the classical technique of Poisson blending.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">There are mainly ﬁve types of pseudo errors: rotation, translation, scaling, distortion, and color, at the time of training to emulate the different errors we face during inference. We remove rotation, applying a heuristic blending technique like Poisson Blending on the heuristically aligned frames, the blending approach fails to produce convincing/photo-realistic results. The neural blending approach learns a non-linear transformation and blending strategy on the given input that cannot be emulated with a heuristic blending approach like Poisson Blending. Poisson blending performs blending really well when the source and the target faces are <b>well aligned </b>and <i>fails to generalize to cases where there’s a difference between the source and the target faces and learning an afﬁne transformation no longer sufﬁces.</i>** [26]</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="II.iv.b."><h3 style="padding-left: 76pt;text-indent: -54pt;text-align: left;"><a name="bookmark16">WHAT IS ‘POISSON’ DISTRIBUTION?</a><a name="bookmark21">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The Poisson distribution is a discrete probability distribution in probability theory and statistics and expresses the probability of the number of occurrences in a certain fixed time unit interval. [27] In our case, Poisson used for smooth out  border-color-gradations  on  the  target  and  source  images  at  the edges/boundary pixels. This stage is important because if miscalculation occurs while computing image, render results might have serious bleeding problems. We also made use of the Laplace filter in our app to minimize errors which explained in next section. Examples that shows how Poisson Blending applied to the images shown in section <i>III.ii.</i></p><p style="padding-left: 182pt;text-indent: 0pt;text-align: left;"><span><img width="192" height="168" alt="Diagram, schematic  Description automatically generated" title="Diagram, schematic  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_038.jpg"/></span></p><p style="padding-top: 7pt;padding-left: 49pt;text-indent: 30pt;line-height: 114%;text-align: left;">Figure 28. Bleeding problems in Poisson image editing arise because of the dependency on the boundary pixels in the target image.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="II.iv.c."><h3 style="padding-left: 76pt;text-indent: -54pt;text-align: left;"><a name="bookmark17">HOW ‘BLENDING’ WORKS?</a><a name="bookmark22">&zwnj;</a></h3></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Gradient images and the Laplace filter in the blending phase allow the process to be both quick and consistent. The gradient of an image measures how it is changing. It provides two pieces of information. The magnitude of the gradient tells us how quickly the image is changing, while the direction of the gradient tells us the direction in which the image is changing most rapidly.</p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Because the gradient has a direction and a magnitude, it is natural to encode this information in a vector. The length of this vector provides the magnitude of the gradient, while its direction gives the gradient direction. Because the gradient may be different at every location, we represent it with a different vector at every image location. [28]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span><img width="338" height="114" alt="A collage of a cat  Description automatically generated with medium confidence" title="A collage of a cat  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_039.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;line-height: 115%;text-align: center;">Figure 29. (a) Intensity image of a cat. (b) a gradient image in the x direction measuring horizontal change in intensity. (c) a gradient image in the y direction measuring vertical change in intensity. ***</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">A Laplacian filter is an edge detector used to compute the second derivatives of an image, measuring the rate at which the first derivatives change. This determines if a change in adjacent pixel values is from an edge or continuous progression.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Laplacian filter kernels usually contain negative values in a cross pattern, centered within the array. The corners are either zero or positive values. The center value can be either negative or positive. It should be remark that there is a disadvantage of Laplace filtering: First derivative operators exaggerate the effects of noise. Second derivatives will exaggerate noise twice as much. [29]</p><p style="padding-left: 206pt;text-indent: 0pt;text-align: left;"><span><img width="131" height="60" alt="A picture containing clock  Description automatically generated" title="A picture containing clock  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_040.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 30.</p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">3x3 kernel for a Laplacian filter which is used in our study.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 138pt;text-indent: 0pt;text-align: left;"><span><img width="314" height="112" alt="A picture containing text  Description automatically generated" title="A picture containing text  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_041.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 31. Laplacian mask example.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li></ol></li><li data-list-text="III."><p class="s4" style="padding-left: 52pt;text-indent: -30pt;text-align: left;"><a name="bookmark23">IMAGE &amp; VIDEO FACE SWAP APPLICATIONS IN MATLAB:</a><a name="bookmark27">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">This study includes 3 different main applications. i and ii used as root function for iii. ****</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l19"><ol id="l20"><li data-list-text="III.I."><h3 style="padding-left: 64pt;text-indent: -42pt;text-align: left;"><a name="bookmark24">REAL-TIME FACE DETECTION AND TRACKING:</a><a name="bookmark28">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Here I create an environment with MATLAB built-in functions and transfer learning concept. (Instead of training, I used existed data that includes needed information) In a given period, machine constantly takes pictures, converts into grayscale, evaluates the face and feature locations and projects to user.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="III.II."><h3 style="padding-left: 70pt;text-indent: -48pt;text-align: left;"><a name="bookmark25">POISSON IMAGE EDITING (FACE&amp;OBJECT SWAP / IMAGE)</a><a name="bookmark29">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Poisson Image Editing (which includes Poisson distribution, Laplace filtering stages) was the second application. Here user crops (creates a mask) the related part of image. (It is not have to be face) The main problem here was scaling issues. I solved that issue with ‘imresize’ command but still for further implementations to get proper solutions, program needs much efficient way to automate the aligning process.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-bottom: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: justify;">Attempt 1</h3><p class="s15" style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><span><img width="116" height="114" alt="A person in a suit  Description automatically generated with medium confidence" title="A person in a suit  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_042.jpg"/></span> <span><img width="111" height="113" alt="A person in a suit and tie  Description automatically generated with medium confidence" title="A person in a suit and tie  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_043.jpg"/></span> <span><img width="108" height="112" alt="A person wearing a headset  Description automatically generated with medium confidence" title="A person wearing a headset  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_044.jpg"/></span> <span><img width="106" height="114" alt="A person wearing a mask  Description automatically generated with low confidence" title="A person wearing a mask  Description automatically generated with low confidence" src="Report_Samet_Bayat_22293730_final_files/Image_045.jpg"/></span> <span><img width="106" height="112" alt="A person in a suit and tie  Description automatically generated with medium confidence" title="A person in a suit and tie  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_046.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 117%;text-align: center;">Figure 32. From left: (a) source, (b) target, (c) mask, (d) mask placed in ROI by user (e) output (<i>imresize </i>function and intermediate steps</p><p style="padding-left: 44pt;text-indent: 0pt;line-height: 11pt;text-align: center;">effects the dimensions &amp; resolutions of output)</p><h3 style="padding-top: 3pt;padding-bottom: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Attempt 2</h3><p class="s15" style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><span><img width="120" height="80" alt="A person walking in the desert  Description automatically generated with medium confidence" title="A person walking in the desert  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_047.jpg"/></span> <span><img width="120" height="80" alt="A snowman in the snow  Description automatically generated" title="A snowman in the snow  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_048.jpg"/></span> <span><img width="67" height="79" alt="A snowman with a yellow scarf  Description automatically generated with medium confidence" title="A snowman with a yellow scarf  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_049.jpg"/></span> <span><img width="121" height="80" alt="A snowman in a desert  Description automatically generated with medium confidence" title="A snowman in a desert  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_050.jpg"/></span> <span><img width="153" height="86" alt="A snowman in a desert  Description automatically generated" title="A snowman in a desert  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_051.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">Figure 33. (a) target, (b) source, (c) extracted source,</p><p style="padding-top: 1pt;padding-left: 154pt;text-indent: 24pt;line-height: 117%;text-align: left;">(d) copy-paste, (e) output source (source): depositphotos.com</p><p style="padding-left: 157pt;text-indent: 15pt;line-height: 114%;text-align: left;">source (target): nytimes.com (NOTE: Rendering lasted 18 mins.)</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="III.III."><h3 style="padding-left: 76pt;text-indent: -54pt;text-align: left;"><a name="bookmark26">VIDEO FACE SWAP APPLICATION</a><a name="bookmark30">&zwnj;</a></h3></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The main purpose of this project is what we did in the iii<span class="s3">rd </span>application. The combination of i<span class="s3">st </span>and ii<span class="s3">nd </span>became an automated video face swap technique here. Steps described in below:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l21"><li data-list-text="a."><p style="padding-left: 40pt;text-indent: -18pt;text-align: left;">Dataset</p><p class="s23" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Dataset Information</p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Num. of Source: 3 (98, 201, 81 frames)</p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Num. of Target: 4 (98, 201, 81, 98* frames)</p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">*Added later to original dataset.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">There are 3 source and target frame sets exist in original dataset. In all samples which are stabilized and aligned, target donors face the lens at a 90- degree angle. We observe that oscillation and zoom in-out are done without disturbing the angle between their faces and the lens (without turning their heads). As target donor, many trials made with various videos, yet results were unexpected. Attempts with wrong results showed at the end of this part.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Unlike  image  editing,  variables  change  constantly  in  video  editing. (Face/feature location, lighting, rotation, etc.) Before the editing process, we know from previous works that aligning and stabilization has critical importance for face swap application. Therefore, both target and source images need preprocessing. The very first step is converting videos to frames. A simple <i>ffmpeg </i>terminal command used for converting. * Later, thanks for M. Afifi’s stabilization tool, 4<span class="s3">th </span>target’s stabilized video (where I am the target donor) obtained about in 5 mins.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-left: 22pt;text-indent: 0pt;text-align: left;">* ffmpeg -t 10 -i ../Movies/sample.mov ../DeepfakeMATLAB/frames/%05d.jpg</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="b."><p style="padding-left: 40pt;text-indent: -18pt;text-align: justify;">Face &amp; Feature Detection</p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Steps used in application i imported here. First, image converted to grayscale for faster detection. Additionally, for more advanced feature extraction, <i>haarcascade_frontalface_alt.xml </i>file from OpenCV is used as transfer learning method.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;">faceDetector = vision.CascadeObjectDetector(<span style=" color: #A709F5;">&#39;haarcascade_frontalface_alt.xml&#39;</span>)</p></li><li data-list-text="c."><p style="padding-top: 3pt;padding-left: 40pt;text-indent: -18pt;text-align: justify;">Blending</p><p class="s5" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">MPB <span class="p">function which developed from application ii, blends target and source with using Poisson distribution and Laplace filters as these equations at part</span></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">II.iv.c. indicated. After computation, function saves blended frame to output folder. Unlike application ii, here since we work with aligned and stabilized dataset, we do not need to rearrange target or source frame. When KLT is added here, the blending process will be applied to the &#39;ROI&#39; in a fully automatic manner during the process.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="d."><p style="padding-left: 40pt;text-indent: -18pt;text-align: justify;">Tracking the Face using KLT</p></li></ol><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Again, like in real-time tracking, machine creates edge points with using eigenvalues and calculates the new head position based on the previous frame. The functions below already explained in section II.II.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: left;">points = detectMinEigenFeatures(rgb2gray(videoFrame), <span style=" color: #A709F5;">&#39;ROI&#39;</span>, bbox); pointTracker = vision.PointTracker(<span style=" color: #A709F5;">&#39;MaxBidirectionalError&#39;</span>, 2);</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">In addition to application i, here we used <i>estimateGeometricTransform </i>function which calculates the translation, rotation, and scale of the tracked face between frames. The results are used to characterize the motion of the face.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: -72pt;line-height: 114%;text-align: left;">[xform, oldInliers, visiblePoints] = estimateGeometricTransform(<span style=" color: #0D00FF;">... </span>oldInliers, visiblePoints, <span style=" color: #A709F5;">&#39;similarity&#39;</span>, <span style=" color: #A709F5;">&#39;MaxDistance&#39;</span>, 4);</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;">[boxEdge(1:2:end), boxEdge(2:2:end)] <span style=" color: #0D00FF;">...</span></p><p style="padding-top: 1pt;padding-left: 76pt;text-indent: 0pt;text-align: left;">= transformPointsForward(xform, boxEdge(1:2:end), boxEdge(2:2:end));</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">During operations, masks and the target frame are constantly refreshed until it reaches the target number of frames. The process can be observed with the simple user interface that pops-up when the code executed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 161pt;text-indent: 0pt;text-align: left;"><span><img width="251" height="108" alt="image" src="Report_Samet_Bayat_22293730_final_files/Image_052.png"/></span></p><p style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 34.</p><p style="padding-top: 1pt;padding-left: 133pt;text-indent: 0pt;line-height: 114%;text-align: center;">A frame from video face swap application. (Target donor: Author.)</p></li><li data-list-text="IV."><p class="s4" style="padding-top: 4pt;padding-left: 46pt;text-indent: -24pt;text-align: left;"><a name="bookmark31">EXEMPLARY APPLICATIONS</a><a name="bookmark35">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l22"><ol id="l23"><li data-list-text="IV.I."><h3 style="padding-left: 58pt;text-indent: -36pt;text-align: left;"><a name="bookmark32">HIGH-RESOLUTION NEURAL FACE SWAPPING FOR VISUAL EFFECTS</a><a name="bookmark36">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Unlike our mid-resolution examples, in 2020, Disney released an Autoencoder &amp; GAN-based method that allows neural face swap for high-resolution videos. * Outputs are quite different. It can be seen on first sight that Disney’s outputs have  unique look which seems familiar us from Disney’s movie world.[7]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 159pt;text-indent: 0pt;text-align: left;"><span><img width="256" height="251" alt="A collage of a person  Description automatically generated with medium confidence" title="A collage of a person  Description automatically generated with medium confidence" src="Report_Samet_Bayat_22293730_final_files/Image_053.jpg"/></span></p><p style="padding-top: 1pt;padding-left: 63pt;text-indent: 0pt;line-height: 115%;text-align: center;">Figure 35. Comparison of face swapping with Disney’s compositing and Poisson methods. From left: (a)target, (b)network output, (c)Poisson blending, (d)Disney’s.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 104pt;text-indent: 0pt;line-height: 112%;text-align: center;">*Let&#39;s note that, for a single relatively high-resolution image in our III.ii. Attempt 2, the process took 18 minutes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="IV.II."><h3 style="padding-left: 64pt;text-indent: -42pt;text-align: left;"><a name="bookmark33">SYNTHETIC FILM DUBBING</a><a name="bookmark37">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Nowadays, in the age of video consumption, many people prefer to watch dubbed content rather than subtitles. Companies like &#39;Flawless AI&#39;, which thinks that following the subtitles disconnects the viewer from the story, aims to overcome this situation by making lip sync based visual translation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 65pt;text-indent: 0pt;text-align: left;"><span><img width="507" height="171" alt="A picture containing text, indoor  Description automatically generated" title="A picture containing text, indoor  Description automatically generated" src="Report_Samet_Bayat_22293730_final_files/Image_054.jpg"/></span></p><p style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">Figure 36. Example of synthetic dubbing.</p></li><li data-list-text="IV.III."><h3 style="padding-top: 3pt;padding-left: 70pt;text-indent: -48pt;text-align: left;"><a name="bookmark34">DEEPFAKE ANCHOR</a><a name="bookmark38">&zwnj;</a></h3></li></ol></ol><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Channel owners are looking for low-budget solutions these days when costs are increasing. &#39;Deepfake anchors&#39; seems to be the new trend for news channels in the field of video processing, where artificial intelligence is in demand. The process, on the other hand, is similar to the stages mentioned in our study, but the masking process is more detailed and audio processing is also included in the process.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="V."><p class="s4" style="padding-left: 40pt;text-indent: -18pt;text-align: left;"><a name="bookmark39">ETHICAL ISSUES</a><a name="bookmark40">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">The vast majority of deepfake applications are used for entertainment (as individual and as industrial) and education, but in some cases, we see this application used maliciously. In the video published recently, the &#39;fake&#39; president who called for his army to &#39;surrender&#39; during the Russia-Ukraine war, and Obama, who was also one of the bureaucratic donors, &#39;saying unspeakable words&#39; are examples of these. For such applications of rapidly developing artificial intelligence, states should keep sanctions tight. Also, some recent applications like FaceApp, ZAO, Reface, etc. violates <i>personal data protection law </i>yet due to delay of international laws, solving such situations requires a long time. (Privacy protocols are now regulated in the mentioned applications.)</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="VI."><p class="s4" style="padding-left: 46pt;text-indent: -24pt;text-align: left;"><a name="bookmark41">CONCLUSION</a><a name="bookmark42">&zwnj;</a></p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">When we complete the applications, we see that some of the results developed as we wanted, others could not reach the desired situation. &#39;Alpha Composition - Transparency Issue&#39;, which is corrected for Image Editing, and our code that requires less trail -error is good for us. For video editing, semi -automation was provided much faster even for the end user. The biggest success here is &#39;Tracking Optimization&#39;. The object followed by random features selected from the first image can now be followed more stable with the help of &#39;Good Features&#39;. At the beginning of the factors behind this success, the &#39;Transfer Learning&#39; method is coming. Thanks to the feature information in the .XML file (which is our &#39;Viola-Jones Algorithm&#39;), we can detect a face by 84.95%.</p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">On the other hand, some situations during the process did not result as desired. The dataset we used for &#39;Feature Training&#39; has been subjected to GPU-operational system restrictions. Although improvements have been made for image editing, which we could perform manually, full automation could not be realized. (We still need to watch the Trial-Error for the target and resource aligning process. The results we improve with the &#39;Good Features to Track&#39; approach are also subject to the Trial-Error process as for image editing. Our code, which is requested to work like an optimized program (can make face swap for each appropriate video) is not yet at this stage. (For this reason, for the appropriate results in video trials, the dataset used by Mahmoud Afifi was used.) For the result obtained with the video added to the existing Dataset (the face of the target author), appropriate results were derived from only 9 seconds of 1 minute 33 seconds footage. Additionally, MATLAB Android/iOS Toolbox allows us to mobilize our application, for further it can be adapted to the new generation phones.</p><p class="s4" style="padding-top: 4pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a name="bookmark43">REFERENCES</a><a name="bookmark44">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l24"><li data-list-text="[1]"><p class="s26" style="padding-left: 41pt;text-indent: -19pt;text-align: left;">https://www.bbc.com/news/world-middle-east-26533994 <span class="s7">Accessed: Dec 15, 2022.</span></p></li><li data-list-text="[2]"><p class="s7" style="padding-top: 1pt;padding-left: 41pt;text-indent: -19pt;text-align: left;">Méliès G. (Director). (1902). <i>Le Voyage dans la Lune </i>[Film]. Star Film Company.</p></li><li data-list-text="[3]"><p class="s26" style="padding-top: 1pt;padding-left: 41pt;text-indent: -19pt;text-align: left;">https://journals.openedition.org/1895/4784 <span class="s7">Accessed: Feb 26, 2022.</span></p></li><li data-list-text="[4]"><p class="s26" style="padding-top: 1pt;padding-left: 41pt;text-indent: -19pt;text-align: left;">https://thesocialshepherd.com/blog/snapchat-statistics <span class="s7">Accessed: Dec 18, 2022.</span></p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">*According to Snapchat, an average of 5+ billion snaps are created every day.</p></li><li data-list-text="[5]"><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: justify;">Habib A, Ali T, Nazir Z, Mahfooz A. Snapchat filters changing young women&#39;s attitudes. Ann Med Surg (Lond). 2022 Sep 17;82:104668.</p></li><li data-list-text="[6]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. 2018. Face2Face: real-time face capture and reenactment of RGB videos. Commun. ACM 62, 1 (January 2019), 96–104.</p></li><li data-list-text="[7]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 117%;text-align: justify;">Naruniec, J. &amp; Helminger, L. &amp; Schroers, C. &amp; Weber, R.M.. (2020). High<span class="s24">‐</span>Resolution Neural Face Swapping for Visual Effects. Computer Graphics Forum. 39. 173-184. 10.1111/cgf.14062.</p></li><li data-list-text="[8]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Afifi, Mahmoud, et al. “Video Face Replacement System Using a Modified Poisson Blending Technique.” 2014 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS), IEEE, 2014, doi:10.1109/ispacs.2014.7024453.</p></li><li data-list-text="[9]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Viola<a href="https://dx.doi.org/10.1109/cvpr.2001.990517" style=" color: black; font-family:&quot;Courier New&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt;" target="_blank">, P.; Jones, M. (2001). </a><a href="https://dx.doi.org/10.1109/cvpr.2001.990517" class="s27" target="_blank">&quot;Rapid object detection using a boosted cascade of </a><span style=" color: #0462C1; font-family:&quot;Courier New&quot;, serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8pt;">simple</span><a href="https://dx.doi.org/10.1109/cvpr.2001.990517" style=" color: #0462C1; font-family:&quot;Courier New&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt;" target="_blank"> </a><span style=" color: #0462C1; font-family:&quot;Courier New&quot;, serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8pt;">features&quot;</span><i>. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001. IEEE Comput. Soc. 1.</i></p></li><li data-list-text="[10]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: justify;">C. Papageorgiou, M. Oren, and T. Poggio. A general framework for object detection. In International Conference on Computer Vision, 1998.</p></li><li data-list-text="[11]"><p class="s7" style="padding-left: 45pt;text-indent: -23pt;text-align: justify;">Freeman and Adelson, 1991; Greenspan et al., 1994</p></li><li data-list-text="[12]"><p class="s26" style="padding-top: 1pt;padding-left: 41pt;text-indent: -19pt;text-align: justify;">https://www.mathworks.com/help/images/ref/integralimage.html <span class="s7">Accessed: Nov 28, 2022.</span></p></li><li data-list-text="[13]"><p style="padding-top: 1pt;padding-left: 62pt;text-indent: -40pt;text-align: justify;"><a href="https://www.mathworks.com/help/vision/ref/vision.cascadeobjectdetector-system-object.html">https://www.mathworks.com/help/vision/ref/vision.cascadeobjectdetector-system-object.html</a></p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: justify;">Accessed: Nov 28, 2022.</p></li><li data-list-text="[14]"><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: left;">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems by Aurelien Geron (pg.201-205)</p></li><li data-list-text="[15]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: left;">Guo, Y., Zhang, L., Hu, Y., He, X., &amp; Gao, J. (2016). MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition. In <i>ECCV 2016</i>.</p></li><li data-list-text="[16]"><p class="s26" style="padding-left: 45pt;text-indent: -23pt;line-height: 9pt;text-align: left;">https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html <span class="s7">Accessed: Dec 1, 2022.</span></p></li><li data-list-text="[17]"><p class="s26" style="padding-top: 1pt;padding-left: 45pt;text-indent: -23pt;text-align: left;">https://github.com/opencv/opencv/tree/master/data/haarcascades<span class="s29"> </span><span class="s7">Accessed: Nov 28, 2022.</span></p></li><li data-list-text="[18]"><p class="s26" style="padding-top: 1pt;padding-left: 41pt;text-indent: -19pt;text-align: left;">https://www.crcv.ucf.edu/wp-content/uploads/2019/03/Lecture-10-KLT.pdf<span class="s29"> </span><span class="s7">Accessed: Nov 30, 2022.</span></p></li><li data-list-text="[19]"><p class="s26" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a href="https://ri.cmu.edu/pub_files/pub3/lucas_bruce_d_1981_2/lucas_bruce_d_1981_2.pdf" class="s30" target="_blank">Bruce D. Lucas and Takeo Kanade. </a><a href="https://ri.cmu.edu/pub_files/pub3/lucas_bruce_d_1981_2/lucas_bruce_d_1981_2.pdf" class="a" target="_blank">An Iterative Image Registration Technique with an </a>Application<a href="https://ri.cmu.edu/pub_files/pub3/lucas_bruce_d_1981_2/lucas_bruce_d_1981_2.pdf" style=" color: #0462C1; font-family:&quot;Courier New&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;" target="_blank"> </a>to Stereo Vision<span class="s7">. </span><span class="s9">International Joint Conference on Artificial Intelligence</span><span class="s7">, pages 674–679, 1981.</span></p></li><li data-list-text="[20]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: left;">Carlo Tomasi and Takeo Kanade. Detection and Tracking of Point Features. <i>Carnegie Mellon University Technical Report CMU-CS-91-132</i>, April 1991.</p></li><li data-list-text="[21]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: left;">Jianbo Shi and Carlo Tomasi. Good Features to Track. <i>IEEE Conference on Computer Vision and Pattern Recognition</i>, pages 593–600, 1994.</p></li><li data-list-text="[22]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: left;">Chris Harris and Mike Stephens (1988). &quot;A Combined Corner and Edge Detector&quot;. Alvey Vision Conference. Vol. 15.</p></li><li data-list-text="[23]"><p class="s26" style="padding-left: 45pt;text-indent: -23pt;text-align: left;">https://www.mathworks.com/help/vision/ref/detectmineigenfeatures.html <span class="s7">Accessed: Dec 5, 2022.</span></p></li><li data-list-text="[24]"><p style="padding-top: 1pt;padding-left: 45pt;text-indent: -23pt;text-align: left;"><a href="http://www.mathworks.com/help/vision/ref/vision.pointtracker-system-object.html" class="s30" target="_blank">https://www.mathworks.com/help/vision/ref/vision.pointtracker-system-object.html</a></p></li><li data-list-text="[25]"><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Ghindawi, Ikhlas &amp; Abdulateef, Sali &amp; Dawood, Amaal &amp; yousif, Intisar. (2020). Modified Alignment Technique Using Matched Important Features. International Journal of Engineering Research and Advanced Technology. 06. 01-06. 10.31695/IJERAT.2020.3592.</p></li><li data-list-text="[26]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Agarwal, Aditya &amp; Sen, Bipasha &amp; Mukhopadhyay, Rudrabha &amp; Namboodiri, Vinay &amp; Jawahar, C.. (2022). FaceOff: A Video-to-Video Face Swapping System.</p></li><li data-list-text="[27]"><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: justify;">Haight, Frank A. (1967). Handbook of the Poisson Distribution. New York, NY, USA: John Wiley &amp; Sons. ISBN 978-0-471-33932-8.</p></li><li data-list-text="[28]"><p class="s7" style="padding-left: 45pt;text-indent: -23pt;text-align: justify;">Jacobs, David. &quot;Image gradients.&quot; Class Notes for CMSC 426 (2005)</p></li><li data-list-text="[29]"><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">R. Haralick and L. Shapiro Computer and Robot Vision, Vol. 1, Addison-Wesley Publishing Company, 1992, pp 346 - 351.</p></li></ol><p class="s4" style="padding-top: 4pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a name="bookmark45">FIGURE REFERENCES</a><a name="bookmark46">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a href="https://upload.wikimedia.org/wikipedia/commons/0/04/Le_Voyage_dans_la_lune.jpg" class="s30" target="_blank">Figure 1. </a>https://upload.wikimedia.org/wikipedia/commons/0/04/Le_Voyage_dans_la_lune.jpg<span class="s29"> </span><span class="s7">Figure 2.</span></p><p class="s26" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: left;">https://towardsdatascience.com/viola-jones-algorithm-and-haar-cascade-classifier-ee3bfb19f7d8<span class="s29"> </span><span class="s7">Figure 3.</span></p><p class="s26" style="padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: left;">https://medium.com/patron-ai/viola-jones-algoritmas%C4%B1-ile-y%C3%BCz-tespiti-t%C3%BCrk%C3%A7e-<a href="https://medium.com/patron-ai/viola-jones-algoritmas%C4%B1-ile-y%C3%BCz-tespiti-t%C3%BCrk%C3%A7e-38ea73c910e3" style=" color: #0462C1; font-family:&quot;Courier New&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;" target="_blank"> </a><a href="https://medium.com/patron-ai/viola-jones-algoritmas%C4%B1-ile-y%C3%BCz-tespiti-t%C3%BCrk%C3%A7e-38ea73c910e3" target="_blank">38ea73c910e3</a></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;text-align: left;">Figure 4 and 5. [13]</p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Figure 7.</p><p class="s26" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: left;">https://levelup.gitconnected.com/artificial-intelligence-how-the-viola-jones-algorithm-help-in-<a href="https://levelup.gitconnected.com/artificial-intelligence-how-the-viola-jones-algorithm-help-in-object-detection-28320596a81c" style=" color: #0462C1; font-family:&quot;Courier New&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;" target="_blank"> </a><a href="https://levelup.gitconnected.com/artificial-intelligence-how-the-viola-jones-algorithm-help-in-object-detection-28320596a81c" target="_blank">object-detection-28320596a81c</a></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: left;">which inspiered from</p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Sanjaya, W. S. Mada &amp; Anggraeni, Dyah &amp; Zakaria, Kiki &amp; Juwardi, Atip &amp; Munawwaroh, Madinatul. (2017). The design of face recognition and tracking for human-robot interaction. 315-320. 10.1109/ICITISEE.2017.8285519.</p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a href="https://mymodernmet.com/free-ai-generated-faces/" class="s30" target="_blank">Figure 8. </a><a href="https://mymodernmet.com/free-ai-generated-faces/" target="_blank">https://mymodernmet.com/free-ai-generated-faces/</a></p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: justify;"><a href="https://mubi.com/tr/cast/alejandro-jodorowsky" class="s30" target="_blank">Figure 10. </a><a href="https://mubi.com/tr/cast/alejandro-jodorowsky" target="_blank">https://mubi.com/tr/cast/alejandro-jodorowsky</a></p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: justify;">Figure                                      11.</p><p class="s26" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: left;">https://en.wikipedia.org/wiki/Solvay_Conference#/media/File:Solvay_conference,_1924.jpg<span class="s29"> </span><span class="s7">Figure 15.</span></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><a href="https://www.youtube.com/watch?v=dBu5BnksHTU">https://www.youtube.com/watch?v=dBu5BnksHTU</a></p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Figure 16. <span class="s26">https://en.wikipedia.org/wiki/Papa_Smurf#/media/File:Papasmurf1.jpg</span></p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a href="https://www.youtube.com/watch?v=6NFsjchCQxc" class="s30" target="_blank">Figure 17. </a><a href="https://www.youtube.com/watch?v=6NFsjchCQxc" target="_blank">https://www.youtube.com/watch?v=6NFsjchCQxc</a></p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a href="https://theailearner.com/2021/09/25/harris-corner-detection/" class="s30" target="_blank">Figure 21. </a><a href="https://theailearner.com/2021/09/25/harris-corner-detection/" target="_blank">https://theailearner.com/2021/09/25/harris-corner-detection/</a></p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 113%;text-align: justify;">Figure 22. Baker, Simon &amp; Matthews, Iain. (2004). Lucas-Kanade 20 Years On: A Unifying Framework Part 1: The Quantity Approximated, the Warp Update Rule, and the Gradient Descent Approximation. International Journal of Computer Vision - IJCV.</p><p class="s26" style="padding-left: 22pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a href="http://amroamroamro.github.io/mexopencv/opencv/generic_corner_detector_demo.html" class="s30" target="_blank">Figure 23. </a>http://amroamroamro.github.io/mexopencv/opencv/generic_corner_detector_demo.html<span class="s29"> </span><span class="s7">Figure 24. Hamouz, Miroslav. (2022). Feature-based affine-invariant detection and localization of faces.</span></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;text-align: left;">Figure 25. Face Donor: Dan Warner, Acting Career Expert Studio.</p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 112%;text-align: left;">Figure 27. Agarwal, Aditya &amp; Sen, Bipasha &amp; Mukhopadhyay, Rudrabha &amp; Namboodiri, Vinay &amp; Jawahar, C.. (2022). FaceOff: A Video-to-Video Face Swapping System.</p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;text-align: left;">Figure 28. [8]</p><p class="s7" style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 113%;text-align: left;">Figure 29. <span class="s26">https://www.wikiwand.com/en/Image_gradient#Media/File:Intensity_image_with_gradient_images.png</span> Figure 31. Yildirim, M., Kacar, F. Adapting Laplacian based filtering in digital image processing to a retina-inspired analog image processing circuit. <i>Analog Integr Circ Sig Process </i>100, 537–545 (2019). https://doi.org/10.1007/s10470-019-01481-3</p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><a href="https://www.nvidia.com/ko-kr/gtc/sessions/developer-conference/" class="s30" target="_blank">Figure 36. </a><a href="https://www.nvidia.com/ko-kr/gtc/sessions/developer-conference/" target="_blank">https://www.nvidia.com/ko-kr/gtc/sessions/developer-conference/</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 20pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="629" height="2" alt="image" src="Report_Samet_Bayat_22293730_final_files/Image_055.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Additional Notes:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 113%;text-align: justify;">* Although the silicon processor-based Mac’s seem to support MATLAB with Roestta 2, I saw during my trials that MATLAB app on ARM based Mac’s still need to much more fixing. (During the study, the native MATLAB <i>beta </i>version for the M1 chip was not used.)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;text-align: left;">After I reached the desired stabilization on coding, during final tests 32 crashes happened.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;text-align: left;">** I did not add or subtract that part of article. Apparently, this process causes similar obstacles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;text-align: left;">*** Gray pixels have a small gradient; black or white pixels have a large gradient.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 22pt;text-indent: 0pt;line-height: 115%;text-align: justify;">**** Application ii and iii derived from a study authored by Michael S. Brown &amp; Mahmoud Afifi, York University.</p></body></html>
